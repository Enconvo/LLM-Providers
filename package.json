{
  "$schema": "https://enconvo.com/schemas/extension.json",
  "name": "llm",
  "version": "1.3.118",
  "description": "Large Language Models (LLM) services providers",
  "title": "LLM Providers",
  "icon": "icon.png",
  "author": "ysnows",
  "license": "MIT",
  "categories": [
    "Provider"
  ],
  "group": "llm",
  "type": "module",
  "commands": [
    {
      "name": "enconvo_ai",
      "title": "Enconvo Cloud Plan",
      "description": "Chat using EnconvoAI which provide LLM service, , learn more : [docs](https://docs.enconvo.com/docs/providers/llm)",
      "icon": "enconvo.png",
      "mode": "provider",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion. Every call will consume points",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "openai/gpt-4o-mini",
          "dataProxy": "llm|enconvo_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "password",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "default",
          "placeholder": "sk-********",
          "visibility": "hidden"
        },
        {
          "name": "anthropicApiKey",
          "description": "OpenAI api key",
          "type": "password",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "default",
          "placeholder": "OpenAI Api Key",
          "visibility": "hidden"
        },
        {
          "name": "apiKey",
          "description": "google api key",
          "type": "password",
          "required": false,
          "title": "google Api Key",
          "default": "default",
          "placeholder": "google Api Key",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_open_ai",
      "title": "OpenAI",
      "description": "Chat with OpenAI , learn more : [openai developers](https://platform.openai.com/docs/overview)",
      "icon": "openai.png",
      "mode": "provider",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gpt-4o-mini",
          "dataProxy": "llm|openai_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "password",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "",
          "defaultProxy": "KEY_OPENAI_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "defaultProxy": "KEY_OPENAI_BASEURL",
          "default": "https://api.openai.com/v1",
          "placeholder": "OpenAI Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "anthropic_models",
      "targetCommand": "llm|fetch_models",
      "title": "Anthropic Models",
      "description": "get anthropic model list",
      "icon": "anthropic.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "type": "textfield",
          "description": "Anthropic models Api Url",
          "required": true,
          "title": "Anthropic models Api Url",
          "default": "https://file.enconvo.com/modles/anthropic.json"
        }
      ]
    },
    {
      "name": "chat_anthropic",
      "title": "Anthropic Claude",
      "description": "Chat with Anthropic Claude, learn more : [anthropic.com](https://www.anthropic.com/)",
      "mode": "provider",
      "icon": "anthropic.png",
      "preferences": [
        {
          "name": "modelName",
          "description": "md: The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "claude-3-5-sonnet-latest",
          "dataProxy": "llm|anthropic_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "anthropicApiKey",
          "description": "Anthropic api key",
          "type": "password",
          "required": false,
          "title": "Claude Api Key",
          "default": "",
          "defaultProxy": "KEY_CLAUDE_APIKEY",
          "placeholder": "sk-ant-********"
        },
        {
          "name": "anthropicApiUrl",
          "description": "Anthropic api base url",
          "type": "textfield",
          "required": false,
          "title": "Anthropic Api Base Url",
          "defaultProxy": "KEY_CLAUDE_BASEURL",
          "default": "https://api.anthropic.com",
          "placeholder": "https://api.anthropic.com",
          "visibility": "show"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_google",
      "title": "Google Gemini AI",
      "description": "Chat with Google Gemini AI, learn more : [Google AI](https://ai.google.dev/)",
      "icon": "google.jpg",
      "mode": "provider",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemini-2.0-flash-exp",
          "data": [
            {
              "title": "Gemini 2.0 Flash Exp",
              "value": "gemini-2.0-flash-exp",
              "context": 1048576,
              "visionEnable": true
            },
            {
              "title": "Gemini 1.5 Flash-8B",
              "value": "gemini-1.5-flash-8b",
              "context": 1048576,
              "visionEnable": true
            },
            {
              "title": "Gemini 1.5 Flash 002",
              "value": "gemini-1.5-flash-002",
              "context": 1048576,
              "visionEnable": true
            },
            {
              "title": "Gemini 1.5 Pro 002",
              "value": "gemini-1.5-pro-002",
              "context": 2097152,
              "visionEnable": true
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "Google AI api key",
          "type": "password",
          "required": false,
          "title": "Google AI Api Key",
          "default": "",
          "defaultProxy": "KEY_GEMINI_APIKEY",
          "placeholder": "AI********"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "azure_openai",
      "title": "Azure OpenAI",
      "description": "Azure OpenAI , learn more : [azure openai](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)",
      "icon": "azure.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "Azure OpenAI api deployment name",
          "type": "dropdown",
          "required": false,
          "title": "Azure OpenAI Api Deployment Name",
          "default": "gpt-4o",
          "dataProxy": "llm|azureai_models"
        },
        {
          "name": "azureOpenAIApiInstanceName",
          "description": "Azure OpenAI api instance name",
          "type": "textfield",
          "required": false,
          "title": "Instance Name",
          "default": "",
          "defaultProxy": "KEY_AZURE_OPENAI_INSTANCE_NAME"
        },
        {
          "name": "azureOpenAIApiVersion",
          "description": "Azure OpenAI api version",
          "type": "textfield",
          "required": false,
          "title": "API Version",
          "defaultProxy": "KEY_AZURE_OPENAI_API_VERSION",
          "default": "2024-02-15-preview"
        },
        {
          "name": "azureOpenAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://azure.microsoft.com/en-us/products/ai-services/openai-service)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_AZURE_OPENAI_APIKEY",
          "placeholder": "Azure OpenAI Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_groq",
      "title": "Groq AI",
      "description": "Chat with Groq ai, learn more : [groq.com/](https://groq.com/)",
      "icon": "groq.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemma2-9b-it",
          "dataProxy": "llm|groq_models"
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://console.groq.com/keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_GROQ_APIKEY",
          "placeholder": "gsk_********"
        },
        {
          "name": "baseUrl",
          "description": "Groq api base url",
          "type": "textfield",
          "required": false,
          "title": "Groq Api Base Url",
          "default": "https://api.groq.com/openai/v1",
          "placeholder": "Groq Api Base Url",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_x_ai",
      "title": "X.AI-GROK",
      "description": "Chat with Grok AI, learn more : [x.ai](https://x.ai/)",
      "icon": "x.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "grok-2-1212",
          "dataProxy": "llm|x_ai_models"
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://docs.x.ai/docs/quickstart#creating-an-api-key)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_XAI_APIKEY",
          "placeholder": "xai-********"
        },
        {
          "name": "baseUrl",
          "description": "X.AI api base url",
          "type": "textfield",
          "required": false,
          "title": "X.AI Api Base Url",
          "default": "https://api.x.ai/v1",
          "placeholder": "X.AI Api Base Url",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_mistral",
      "title": "Mistral AI",
      "description": "Chat with Mistral AI, learn more : [mistral.ai](https://mistral.ai/)",
      "icon": "mistral.jpg",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "md:The model to generate the completion. \n\n ## Pricing and rate limits\n\n**Pay-as-you-go**\n\nThe prices listed below are exclusive of VAT.\n\n### Chat Completions API\n\n| Model           | Endpoint                | Input (USD)         | Output (USD)         |\n|-----------------|-------------------------|---------------------|----------------------|\n| Mistral 7B      | open-mistral-7b         | 0.25$ / 1M tokens   | 0.25$ / 1M tokens    |\n| Mixtral 8x7B    | open-mixtral-8x7b       | 0.7$ / 1M tokens    | 0.7$ / 1M tokens     |\n| Mistral Small   | mistral-small-latest     | 2$ / 1M tokens     | 6$ / 1M tokens       |\n| Mistral Medium  | mistral-medium-latest    | 2.7$ / 1M tokens   | 8.1$ / 1M tokens     |\n| Mistral Large   | mistral-large-latest     | 8$ / 1M tokens     | 24$ / 1M tokens      |\n",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "mistral-small-latest",
          "dataProxy": "llm|mistral_models"
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://console.mistral.ai/api-keys/)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_MISTRAL_APIKEY",
          "placeholder": "Mistral AI Api Key"
        },
        {
          "name": "baseUrl",
          "description": "Mixtral api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "https://api.mistral.ai/v1",
          "defaultProxy": "KEY_MISTRAL_BASEURL",
          "placeholder": "Mixtral Api Base Url"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_ollama",
      "title": "Ollama",
      "description": "Chat with Ollama, learn more : [ollama.ai](https://ollama.ai/)",
      "icon": "ollama.png",
      "mode": "provider",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama2:latest",
          "dataProxy": "llm|ollama_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "baseUrl",
          "description": "Ollama api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "http://127.0.0.1:11434",
          "placeholder": "http://127.0.0.1:11434",
          "defaultProxy": "KEY_OLLAMA_BASEURL"
        }
      ]
    },
    {
      "name": "chat_cohere",
      "title": "Cohere AI",
      "description": "Chat using [Cohere](cohere.com), get [API KEY](https://dashboard.cohere.com/)",
      "icon": "cohere.jpg",
      "mode": "provider",
      "preferences": [
        {
          "name": "model",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "command-r-plus",
          "dataProxy": "llm|cohere_models"
        },
        {
          "name": "apiKey",
          "description": "How to get api key? [ðŸ”‘here](https://dashboard.cohere.com/api-keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_COHERE_APIKEY",
          "placeholder": "API Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_perplexity",
      "title": "Perplexity",
      "description": "Chat with Perplexity, learn more: [perplexity.ai](https://www.perplexity.ai/settings/api)",
      "icon": "perplexity.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama-3.1-sonar-small-128k-online",
          "dataProxy": "llm|perplexity_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://www.perplexity.ai/settings/api)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_PERPLEXITY_APIKEY",
          "placeholder": "pplx-********"
        },
        {
          "name": "frequencyPenalty",
          "description": "frequencyPenalty",
          "type": "textfield",
          "required": false,
          "title": "frequencyPenalty",
          "default": "1",
          "visibility": "hidden"
        },
        {
          "name": "baseUrl",
          "description": "Perplexity api base url",
          "type": "textfield",
          "required": false,
          "title": "Perplexity Api Base Url",
          "default": "https://api.perplexity.ai",
          "placeholder": "Perplexity Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "deepseek",
      "title": "Deep Seek",
      "description": "Chat with DeepSeek , learn more : [DeepSeek developers](https://platform.deepseek.com/)",
      "icon": "deepseek.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "deepseek-chat",
          "data": [
            {
              "title": "Deepseek Chat",
              "value": "deepseek-chat",
              "context": 32000,
              "inputPrice": 0.00014,
              "outputPrice": 0.00028
            },
            {
              "title": "Deepseek Coder",
              "value": "deepseek-coder",
              "context": 16000,
              "inputPrice": 0.00014,
              "outputPrice": 0.00028
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://platform.deepseek.com/api_keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_DEEPSEEK_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.deepseek.com",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "siliconflow",
      "title": "SiliconFlow",
      "description": "Chat with SiliconFlow , learn more : [SiliconFlow developers](https://siliconflow.cn/)",
      "icon": "siliconflow.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "Qwen/Qwen2-7B-Instruct",
          "dataProxy": "llm|silliconflow_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://cloud.siliconflow.cn/account/ak)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_SILICONFLOW_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.siliconflow.cn/v1",
          "placeholder": "Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_lm_studio",
      "title": "LM Studio",
      "description": "Chat with LM Studio, learn more : [lmstudio.ai](https://lmstudio.ai/)",
      "icon": "lm_studio.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "baseUrl",
          "description": "LM Studio api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "http://127.0.0.1:1234/v1",
          "defaultProxy": "KEY_LMSTUDIO_BASEURL",
          "placeholder": "LM Studio Api Base Url"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "default",
          "visibility": "hidden"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://www.perplexity.ai/settings/api)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "default",
          "placeholder": "pplx-********",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_openrouter",
      "title": "OpenRouter",
      "description": "Chat with OpenRouter, learn more: [openrouter.ai](https://openrouter.ai/)",
      "icon": "openrouter.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "anthropic/claude-3.5-sonnet",
          "data": [
            {
              "title": "Openai Gpt-3.5-turbo",
              "value": "openai/gpt-3.5-turbo",
              "context": 16385,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "toolUse": true
            }
          ],
          "dataProxy": "llm|openrouter_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenRouter api key",
          "type": "password",
          "required": false,
          "title": "OpenRouter Api Key",
          "default": "",
          "defaultProxy": "KEY_OPENROUTER_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenRouter api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenRouter Api Base Url",
          "default": "https://openrouter.ai/api/v1",
          "placeholder": "OpenRouter Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_fireworks",
      "title": "Fireworks",
      "description": "Chat with Fireworks, learn more : [fireworks.ai](https://fireworks.ai), get all models from [fireworks models](https://fireworks.ai/models/fireworks/mixtral-8x7b-instruct)",
      "icon": "fireworks.jpg",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "accounts/fireworks/models/llama-v3p2-11b-vision-instruct",
          "dataProxy": "llm|fireworks_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://fireworks.ai/account/api-keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_FIREWORKS_APIKEY",
          "placeholder": "API Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.fireworks.ai/inference/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_cloudflare",
      "title": "Cloudflare Workers AI",
      "description": "Chat with Cloudflare Workers AI, learn more : [workers-ai](https://developers.cloudflare.com/workers-ai/)",
      "icon": "cloudflare.png",
      "mode": "provider",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "@cf/meta/llama-2-7b-chat-int8",
          "dataProxy": "llm|cloudflare_models"
        },
        {
          "name": "cloudflareAccountId",
          "description": "Cloudflare AccountID",
          "type": "textfield",
          "required": false,
          "title": "Cloudflare AccountID",
          "default": "",
          "defaultProxy": "KEY_CLOUDFLARE_ACCOUNTID",
          "placeholder": "Cloudflare AccountID"
        },
        {
          "name": "cloudflareApiToken",
          "description": "Cloudflare API Token",
          "type": "password",
          "required": false,
          "title": "Cloudflare API Token",
          "default": "",
          "defaultProxy": "KEY_CLOUDFLARE_APIKEY",
          "placeholder": "Cloudflare API Token"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_together_ai",
      "title": "Together AI",
      "description": "Chat with together.ai, learn more : [together.ai](https://docs.together.ai/docs/quickstart), get chat models from [together models](https://docs.together.ai/docs/inference-models)",
      "icon": "together.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "meta-llama/Llama-3.2-3B-Instruct-Turbo",
          "dataProxy": "llm|together_models"
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://docs.together.ai/docs/quickstart)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_TOGETHER_APIKEY",
          "placeholder": "Together Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.together.xyz/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_moonshot_ai",
      "title": "MoonShot AI",
      "description": "Chat with moonshot ai, learn more : [moonshot.cn](https://www.moonshot.cn/)",
      "icon": "moonshot.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "moonshot-v1-8k",
          "data": [
            {
              "title": "moonshot-v1-8k",
              "value": "moonshot-v1-8k"
            },
            {
              "title": "moonshot-v1-32k",
              "value": "moonshot-v1-32k"
            },
            {
              "title": "moonshot-v1-128k",
              "value": "moonshot-v1-128k"
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "MOONSHOT_API_KEY",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_MOONSHOT_APIKEY",
          "placeholder": "MoonShot Api Key"
        },
        {
          "name": "baseUrl",
          "description": "MoonShot api base url",
          "type": "textfield",
          "required": false,
          "title": "MoonShot Api Base Url",
          "default": "https://api.moonshot.cn/v1",
          "placeholder": "MoonShot Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "ollama_models",
      "title": "Ollama Models",
      "description": "get ollama model list",
      "icon": "ollama.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "baseUrl",
          "description": "Ollama api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "http://127.0.0.1:11434",
          "placeholder": "http://127.0.0.1:11434",
          "defaultProxy": "KEY_OLLAMA_BASEURL"
        }
      ]
    },
    {
      "name": "openrouter_models",
      "targetCommand": "llm|fetch_models",
      "title": "openrouter Models",
      "description": "get openrouter model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "openrouter api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/openrouter/v1/models"
        }
      ]
    },
    {
      "name": "perplexity_models",
      "targetCommand": "llm|fetch_models",
      "title": "perplexity Models",
      "description": "get perplexity model list",
      "icon": "perplexity.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "title": "Url",
          "description": "perplexity api url",
          "type": "textfield",
          "required": false,
          "default": "https://file.enconvo.com/modles/perplexity.json"
        }
      ]
    },
    {
      "name": "groq_models",
      "title": "groq Models",
      "description": "get groq model list",
      "icon": "groq.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://console.groq.com/keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "llm|chat_groq",
          "placeholder": "gsk_********"
        },
        {
          "name": "baseUrl",
          "description": "Groq api base url",
          "type": "textfield",
          "required": false,
          "title": "Groq Api Base Url",
          "default": "https://api.groq.com/openai/v1",
          "placeholder": "Groq Api Base Url",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "x_ai_models",
      "title": "X.AI Models",
      "description": "get X.AI model list",
      "icon": "x_ai.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://docs.x.ai/docs/quickstart#creating-an-api-key)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "llm|chat_x_ai",
          "placeholder": "xai-********"
        },
        {
          "name": "baseUrl",
          "description": "X.AI api base url",
          "type": "textfield",
          "required": false,
          "title": "X.AI Api Base Url",
          "default": "https://api.x.ai/v1",
          "placeholder": "X.AI Api Base Url",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "mistral_models",
      "title": "Mistral Models",
      "description": "get Mistral model list",
      "icon": "mistral.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://console.mistral.ai/api-keys/)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "llm|chat_mistral",
          "placeholder": "Mistral AI Api Key"
        },
        {
          "name": "baseUrl",
          "description": "Mixtral api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "https://api.mistral.ai/v1",
          "defaultProxy": "KEY_MISTRAL_BASEURL",
          "placeholder": "Mixtral Api Base Url"
        }
      ]
    },
    {
      "name": "together_models",
      "targetCommand": "llm|fetch_models",
      "title": "together Models",
      "description": "get together model list",
      "icon": "together.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "title": "Url",
          "name": "url",
          "description": "together api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/together/v1/models"
        }
      ]
    },
    {
      "name": "fireworks_models",
      "targetCommand": "llm|fetch_models",
      "title": "fireworks Models",
      "description": "get fireworks model list",
      "icon": "fireworks.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "title": "Url",
          "name": "url",
          "description": "fireworks api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/fireworks/v1/models"
        }
      ]
    },
    {
      "name": "fetch_models",
      "title": "fetch Models",
      "description": "get models list",
      "icon": "together.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "enconvo_models",
      "targetCommand": "llm|fetch_models",
      "title": "enconvo Models",
      "description": "get enconvo model list",
      "icon": "enconvo.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "enconvo api url",
          "type": "textfield",
          "required": false,
          "default": "https://file.enconvo.com/modles/enconvo.json"
        }
      ]
    },
    {
      "name": "openai_models",
      "title": "openai Models",
      "description": "get openai model list",
      "icon": "openai.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "password",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "",
          "defaultProxy": "llm|chat_open_ai",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "defaultProxy": "llm|chat_open_ai",
          "default": "https://api.openai.com/v1",
          "placeholder": "OpenAI Api Base Url"
        }
      ]
    },
    {
      "name": "azureai_models",
      "title": "azureai Models",
      "targetCommand": "llm|fetch_models",
      "description": "get azureai model list",
      "icon": "azureai.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "openai api url",
          "type": "textfield",
          "required": false,
          "default": "https://file.enconvo.com/modles/openai.json"
        }
      ]
    },
    {
      "name": "cloudflare_models",
      "title": "cloudflare Models",
      "targetCommand": "llm|fetch_models",
      "description": "get cloudflare model list",
      "icon": "cloudflare.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "cloudflare api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/cloudflare/v1/models"
        }
      ]
    },
    {
      "name": "cohere_models",
      "title": "cohere Models",
      "targetCommand": "llm|fetch_models",
      "description": "get cohere model list",
      "icon": "cohere.jpg",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "title": "Url",
          "description": "cohere api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/cohere/v1/models"
        }
      ]
    },
    {
      "name": "chat_yii",
      "title": "Yi",
      "description": "Chat with Yi , learn more : [Yi developers](https://platform.lingyiwanwu.com/)",
      "icon": "yi.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "yi-34b-chat-0205",
          "data": [
            {
              "title": "yi-large",
              "value": "yi-large",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-medium",
              "value": "yi-medium",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-vision",
              "value": "yi-vision",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-large-turbo",
              "value": "yi-large-turbo",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-34b-chat-0205",
              "value": "yi-34b-chat-0205",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-34b-chat-200k",
              "value": "yi-34b-chat-200k",
              "context": 200000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "maxTokens": 4000
            },
            {
              "title": "yi-vl-plus",
              "value": "yi-vl-plus",
              "context": 4000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "visionEnable": true,
              "maxTokens": 2000
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "api key",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_YI_APIKEY",
          "placeholder": "API Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.lingyiwanwu.com/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_dashscope",
      "title": "DashScope",
      "description": "Chat with é˜¿é‡Œäº‘ç™¾ç‚¼ , learn more : [é˜¿é‡Œäº‘ç™¾ç‚¼](https://bailian.console.aliyun.com/#/home)",
      "icon": "dashscope.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "qwen-max-latest",
          "data": [
            {
              "title": "Qwen-Max-Latest",
              "value": "qwen-max-latest",
              "context": 32000
            },
            {
              "title": "Qwen-Plus-Latest",
              "value": "qwen-plus-latest",
              "context": 128000
            },
            {
              "title": "Qwen-Turbo-Latest",
              "value": "qwen-turbo-latest",
              "context": 128000
            },
            {
              "title": "Qwen-VL-Max-Latest",
              "value": "qwen-vl-max-latest",
              "context": 8000,
              "visionEnable": true
            },
            {
              "title": "Qwen-VL-Max-Plus",
              "value": "qwen-vl-max-plus",
              "context": 8000,
              "visionEnable": true
            },
            {
              "title": "Qwen2.5-72B",
              "value": "qwen2.5-72b-instruct",
              "context": 128000
            },
            {
              "title": "Qwen2.5-32B",
              "value": "qwen2.5-32b-instruct",
              "context": 128000
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://bailian.console.aliyun.com/#/home)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_DASHSCOPE_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://dashscope.aliyuncs.com/compatible-mode/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_sambanova",
      "title": "Sambanova AI",
      "description": "Chat with sambanova.ai, learn more : [sambanova.ai](https://cloud.sambanova.ai/pricing)",
      "icon": "sambanova.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "Meta-Llama-3.2-3B-Instruct",
          "data": [
            {
              "title": "Meta-Llama-3.1-8B-Instruct",
              "value": "Meta-Llama-3.1-8B-Instruct",
              "context": 16000
            },
            {
              "title": "Meta-Llama-3.1-70B-Instruct",
              "value": "Meta-Llama-3.1-70B-Instruct",
              "context": 64000
            },
            {
              "title": "Meta-Llama-3.1-405B-Instruct",
              "value": "Meta-Llama-3.1-405B-Instruct",
              "context": 4096
            },
            {
              "title": "Meta-Llama-3.2-1B-Instruct",
              "value": "Meta-Llama-3.2-1B-Instruct",
              "context": 4096
            },
            {
              "title": "Meta-Llama-3.2-3B-Instruct",
              "value": "Meta-Llama-3.2-3B-Instruct",
              "context": 4096
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://cloud.sambanova.ai/apis)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_SAMBANOVA_APIKEY",
          "placeholder": "********"
        },
        {
          "name": "baseUrl",
          "description": "Perplexity api base url",
          "type": "textfield",
          "required": false,
          "title": "Perplexity Api Base Url",
          "default": "https://api.sambanova.ai/v1",
          "placeholder": "Perplexity Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_cerebras",
      "title": "Cerebras AI",
      "description": "Chat with cerebras.ai, learn more : [cerebras.ai](https://cerebras.ai/)",
      "icon": "cerebras.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama3.1-8b",
          "data": [
            {
              "title": "Llama 3.1 8B",
              "value": "llama3.1-8b",
              "context": 8192
            },
            {
              "title": "Llama 3.1 70B",
              "value": "llama3.1-70b",
              "context": 8192
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://inference-docs.cerebras.ai/openai)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_CEREBRAS_APIKEY",
          "placeholder": "********"
        },
        {
          "name": "baseUrl",
          "description": "Perplexity api base url",
          "type": "textfield",
          "required": false,
          "title": "Perplexity Api Base Url",
          "default": "https://api.cerebras.ai/v1",
          "placeholder": "Perplexity Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_arli",
      "title": "Arli AI",
      "description": "Chat with arli.ai, learn more : [arliai.com](https://arliai.com/)",
      "icon": "arli.png",
      "mode": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "(TRIAL) Llama-3.1-70B-ArliAI-RPMax-v1.1",
          "dataProxy": "llm|arli_models"
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://arliai.com/)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_ARLI_APIKEY",
          "placeholder": "Arli Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "baseUrl",
          "description": "Arli api base url",
          "type": "textfield",
          "required": false,
          "title": "Arli Api Base Url",
          "default": "https://api.arliai.com/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "silliconflow_models",
      "title": "silliconflow_models",
      "targetCommand": "llm|fetch_models",
      "description": "get silliconflow model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "silliconflow api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/siliconflow/v1/models"
        }
      ]
    },
    {
      "name": "arli_models",
      "title": "arli_models",
      "targetCommand": "llm|fetch_models",
      "description": "get arli model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "arli api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/arli/v1/models"
        }
      ]
    }
  ],
  "dependencies": {
    "@anthropic-ai/sdk": "^0.32.1",
    "@enconvo/api": "link:/Users/ysnows/Documents/Project/enconvo.nodejs/enconvo_api",
    "@google/generative-ai": "^0.21.0",
    "@langchain/anthropic": "^0.3.9",
    "@langchain/cloudflare": "^0.1.0",
    "@langchain/cohere": "^0.3.1",
    "@langchain/community": "^0.3.19",
    "@langchain/core": "^0.3.23",
    "@langchain/google-genai": "^0.1.5",
    "@langchain/google-vertexai": "^0.1.4",
    "@langchain/groq": "^0.1.2",
    "@langchain/mistralai": "^0.2.0",
    "@langchain/openai": "^0.3.14",
    "langchain": "^0.3.7",
    "langsmith": "^0.2.13",
    "ollama": "^0.5.11",
    "openai": "^4.76.3"
  },
  "devDependencies": {
    "@types/node": "^22.10.2",
    "eslint": "^9.17.0",
    "prettier": "^3.4.2",
    "tsup": "^8.3.5",
    "typescript": "^5.7.2"
  },
  "scripts": {
    "lint": "eslint src",
    "lint:fix": "npm run lint --fix",
    "format": "prettier --write \"**/*.ts\"",
    "format:check": "prettier --list-different \"**/*.ts\"",
    "build": "enconvo",
    "dev": "enconvo --dev"
  },
  "minAppVersion": "1.8.8"
}
