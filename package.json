{
  "$schema": "https://enconvo.com/schemas/extension.json",
  "name": "llm",
  "version": "1.3.257",
  "description": "Large Language Models (LLM) services providers",
  "title": "AI Model Providers",
  "icon": "icon.png",
  "author": "ysnows",
  "license": "MIT",
  "categories": [
    "Provider"
  ],
  "group": "llm",
  "type": "module",
  "commands": [
    {
      "name": "enconvo_ai",
      "title": "Enconvo Cloud Plan",
      "description": "Chat using EnconvoAI which provide LLM service, , learn more : [docs](https://docs.enconvo.com/docs/providers/llm)",
      "icon": "enconvo.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "enconvo_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|enconvo_ai"
            ]
          },
          "title": "Credential Provider",
          "visibility": "hidden"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion. Every call will consume points",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "anthropic/claude-sonnet-4-20250514",
          "dataProxy": "llm|enconvo_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "google_search",
          "description": "Grounding with Google Search connects the Gemini model to real-time web content and works with all available languages. This allows Gemini to provide more accurate answers and cite verifiable sources beyond its knowledge cutoff.",
          "type": "checkbox",
          "required": false,
          "title": "Google Search Tool",
          "default": false,
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash-lite",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "url_context",
          "description": "The URL context tool lets you provide additional context to the models in the form of URLs. By including URLs in your request, the model will access the content from those pages (as long as it's not a URL type listed in the limitations section) to inform and enhance its response.",
          "type": "checkbox",
          "required": false,
          "title": "URL Context Tool",
          "default": false,
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash-lite",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "claude_thinking",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Extended thinking",
          "default": "disabled",
          "data": [
            {
              "title": "Disabled",
              "value": "disabled",
              "description": "Model does not think"
            },
            {
              "title": "Minimal",
              "value": "1024",
              "description": "Thinking budget tokens: 1024"
            },
            {
              "title": "Low",
              "value": "2048",
              "description": "Thinking budget tokens: 2048"
            },
            {
              "title": "Medium",
              "value": "5120",
              "description": "Thinking budget tokens: 5120"
            },
            {
              "title": "High",
              "value": "10240",
              "description": "Thinking budget tokens: 10240"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "anthropic/claude-opus-4-1-20250805",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "anthropic/claude-opus-4-20250514",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "anthropic/claude-sonnet-4-20250514",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "anthropic/claude-3-7-sonnet-20250219",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "low",
          "data": [
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "openai/o1-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o1",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o1-pro",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o3-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o3",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o4-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "deepseek-ai/DeepSeek-R1",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai-oss/gpt-oss-20b",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai-oss/gpt-oss-120b",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "reasoning_effort_new",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "minimal",
          "data": [
            {
              "title": "minimal",
              "value": "minimal",
              "description": "Favors speed and economical token usage with minimal reasoning"
            },
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "openai/gpt-5-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/gpt-5",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/gpt-5-nano",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "gemini_thinking_pro",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Gemini thinking",
          "default": "auto",
          "data": [
            {
              "title": "Auto",
              "value": "auto",
              "description": "Model decides when and how much to think"
            },
            {
              "title": "Minimal",
              "value": "128",
              "description": "Thinking budgets: 128"
            },
            {
              "title": "Low",
              "value": "1024",
              "description": "Thinking budgets: 1024"
            },
            {
              "title": "Medium",
              "value": "10000",
              "description": "Thinking budgets: 10000"
            },
            {
              "title": "High",
              "value": "30000",
              "description": "Thinking budgets: 30000"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "gemini_thinking",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Gemini thinking",
          "default": "disabled",
          "data": [
            {
              "title": "Disabled",
              "value": "disabled",
              "description": "Model does not think"
            },
            {
              "title": "Auto",
              "value": "auto",
              "description": "Model decides when and how much to think"
            },
            {
              "title": "Minimal",
              "value": "512",
              "description": "Thinking budgets: 512"
            },
            {
              "title": "Low",
              "value": "1024",
              "description": "Thinking budgets: 1024"
            },
            {
              "title": "Medium",
              "value": "10000",
              "description": "Thinking budgets: 10000"
            },
            {
              "title": "High",
              "value": "20000",
              "description": "Thinking budgets: 20000"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash-lite",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "chat_open_ai",
      "title": "OpenAI",
      "description": "Chat with OpenAI , learn more : [openai developers](https://platform.openai.com/docs/overview)",
      "icon": "openai.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "open_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|open_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gpt-5",
          "dataProxy": "llm|openai_models",
          "dataCustomTitle": "Add Custom Model"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "low",
          "data": [
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "o1-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o1",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o1-pro",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o3-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o3",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o4-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gpt-5-codex",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "reasoning_effort_new",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "minimal",
          "data": [
            {
              "title": "minimal",
              "value": "minimal",
              "description": "Favors speed and economical token usage with minimal reasoning"
            },
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "gpt-5-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gpt-5",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gpt-5-nano",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "chat_qwen",
      "title": "Qwen",
      "description": "Chat with Qwen , learn more : [qwen developers](https://www.alibabacloud.com/help/en/model-studio/use-qwen-by-calling-api)",
      "icon": "qwen.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "qwen",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|qwen"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "qwen3-coder-plus",
          "dataProxy": "llm|qwen_models",
          "dataCustomTitle": "Add Custom Model"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "low",
          "data": [
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "o1-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o1",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o1-pro",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o3-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o3",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o4-mini",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "anthropic_models",
      "title": "Anthropic Models",
      "description": "get anthropic model list",
      "commandType": "function_command",
      "icon": "anthropic.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "anthropic",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|anthropic"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "chat_anthropic",
      "title": "Anthropic",
      "description": "Chat with Anthropic Claude, learn more : [anthropic.com](https://www.anthropic.com/)",
      "mode": "no-view",
      "commandType": "provider",
      "icon": "anthropic.png",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "anthropic",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|anthropic"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "md: The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "claude-sonnet-4-20250514",
          "dataProxy": "llm|anthropic_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            }
          ]
        },
        {
          "name": "claude_thinking",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Extended thinking",
          "default": "disabled",
          "data": [
            {
              "title": "Disabled",
              "value": "disabled",
              "description": "Model does not think"
            },
            {
              "title": "Minimal",
              "value": "1024",
              "description": "Thinking budget tokens: 1024"
            },
            {
              "title": "Low",
              "value": "2048",
              "description": "Thinking budget tokens: 2048"
            },
            {
              "title": "Medium",
              "value": "5120",
              "description": "Thinking budget tokens: 5120"
            },
            {
              "title": "High",
              "value": "10240",
              "description": "Thinking budget tokens: 10240"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "claude-opus-4-1-20250805",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "claude-opus-4-20250514",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "claude-sonnet-4-20250514",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "claude-3-7-sonnet-20250219",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "chat_google",
      "title": "Google Gemini AI",
      "description": "Chat with Google Gemini AI, learn more : [Google AI](https://ai.google.dev/)",
      "icon": "google.jpg",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "gemini",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|gemini"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemini-2.5-flash",
          "dataProxy": "llm|gemini_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 0,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "google_search",
          "description": "Grounding with Google Search connects the Gemini model to real-time web content and works with all available languages. This allows Gemini to provide more accurate answers and cite verifiable sources beyond its knowledge cutoff.",
          "type": "checkbox",
          "required": false,
          "title": "Google Search Tool",
          "default": false,
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gemini-2.5-flash-lite",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "url_context",
          "description": "The URL context tool lets you provide additional context to the models in the form of URLs. By including URLs in your request, the model will access the content from those pages (as long as it's not a URL type listed in the limitations section) to inform and enhance its response.",
          "type": "checkbox",
          "required": false,
          "title": "URL Context Tool",
          "default": false,
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gemini-2.5-flash-lite",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "gemini_thinking_pro",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Gemini thinking",
          "default": "auto",
          "data": [
            {
              "title": "Auto",
              "value": "auto",
              "description": "Model decides when and how much to think"
            },
            {
              "title": "Minimal",
              "value": "128",
              "description": "Thinking budgets: 128"
            },
            {
              "title": "Low",
              "value": "1024",
              "description": "Thinking budgets: 1024"
            },
            {
              "title": "Medium",
              "value": "10000",
              "description": "Thinking budgets: 10000"
            },
            {
              "title": "High",
              "value": "30000",
              "description": "Thinking budgets: 30000"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "gemini_thinking",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Gemini thinking",
          "default": "disabled",
          "data": [
            {
              "title": "Disabled",
              "value": "disabled",
              "description": "Model does not think"
            },
            {
              "title": "Auto",
              "value": "auto",
              "description": "Model decides when and how much to think"
            },
            {
              "title": "Minimal",
              "value": "512",
              "description": "Thinking budgets: 512"
            },
            {
              "title": "Low",
              "value": "1024",
              "description": "Thinking budgets: 1024"
            },
            {
              "title": "Medium",
              "value": "10000",
              "description": "Thinking budgets: 10000"
            },
            {
              "title": "High",
              "value": "20000",
              "description": "Thinking budgets: 20000"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gemini-2.5-flash-lite",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "deepseek",
      "title": "Deep Seek",
      "description": "Chat with DeepSeek , learn more : [DeepSeek developers](https://platform.deepseek.com/)",
      "icon": "deepseek.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "deepseek",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|deepseek"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "deepseek-chat",
          "dataProxy": "llm|deepseek_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 0.6,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "low",
          "data": [
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "deepseek-reasoner",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "chat_groq",
      "title": "Groq AI",
      "description": "Chat with Groq ai, learn more : [groq.com/](https://groq.com/)",
      "icon": "groq.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "groq",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|groq"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemma2-9b-it",
          "dataProxy": "llm|groq_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "low",
          "data": [
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "openai/gpt-oss-120b",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/gpt-oss-20b",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "azure_openai",
      "title": "Azure OpenAI",
      "description": "Azure OpenAI , learn more : [azure openai](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)",
      "icon": "azure.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "azure_openai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|azure_openai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "Azure OpenAI api deployment name",
          "type": "dropdown",
          "required": false,
          "title": "Azure OpenAI Api Deployment Name",
          "default": "gpt-5-mini",
          "dataProxy": "llm|azureai_models"
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "low",
          "data": [
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "o1-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o1",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o1-pro",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o3-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o3",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o4-mini",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "reasoning_effort_new",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "minimal",
          "data": [
            {
              "title": "minimal",
              "value": "minimal",
              "description": "Favors speed and economical token usage with minimal reasoning"
            },
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "gpt-5-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gpt-5",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gpt-5-nano",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_x_ai",
      "title": "X.AI-GROK",
      "description": "Chat with Grok AI, learn more : [x.ai](https://x.ai/)",
      "icon": "x.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "x_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|x_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "grok-4-0709",
          "dataProxy": "llm|x_ai_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_mistral",
      "title": "Mistral AI",
      "description": "Chat with Mistral AI, learn more : [mistral.ai](https://mistral.ai/)",
      "icon": "mistral.jpg",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "mistral",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|mistral"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "md:The model to generate the completion. \n\n ## Pricing and rate limits\n\n**Pay-as-you-go**\n\nThe prices listed below are exclusive of VAT.\n\n### Chat Completions API\n\n| Model           | Endpoint                | Input (USD)         | Output (USD)         |\n|-----------------|-------------------------|---------------------|----------------------|\n| Mistral 7B      | open-mistral-7b         | 0.25$ / 1M tokens   | 0.25$ / 1M tokens    |\n| Mixtral 8x7B    | open-mixtral-8x7b       | 0.7$ / 1M tokens    | 0.7$ / 1M tokens     |\n| Mistral Small   | mistral-small-latest     | 2$ / 1M tokens     | 6$ / 1M tokens       |\n| Mistral Medium  | mistral-medium-latest    | 2.7$ / 1M tokens   | 8.1$ / 1M tokens     |\n| Mistral Large   | mistral-large-latest     | 8$ / 1M tokens     | 24$ / 1M tokens      |\n",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "mistral-small-latest",
          "dataProxy": "llm|mistral_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_ollama",
      "title": "Ollama",
      "description": "Chat with Ollama, learn more : [ollama.ai](https://ollama.ai/)",
      "icon": "ollama.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "ollama",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|ollama"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama2:latest",
          "dataProxy": "llm|ollama_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_cohere",
      "title": "Cohere AI",
      "description": "Chat using [Cohere](cohere.com), get [API KEY](https://dashboard.cohere.com/)",
      "icon": "cohere.jpg",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "cohere",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|cohere"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "model",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "command-r",
          "dataProxy": "llm|cohere_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_perplexity",
      "title": "Perplexity",
      "description": "Chat with Perplexity, learn more: [perplexity.ai](https://www.perplexity.ai/settings/api)",
      "icon": "perplexity.png",
      "mode": "no-view",
      "commandType": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "perplexity",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|perplexity"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "sonar",
          "dataProxy": "llm|perplexity_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "frequencyPenalty",
          "description": "frequencyPenalty",
          "type": "textfield",
          "required": false,
          "title": "frequencyPenalty",
          "default": "1",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "siliconflow",
      "title": "SiliconFlow",
      "description": "Chat with SiliconFlow , learn more : [SiliconFlow developers](https://siliconflow.cn/)",
      "icon": "siliconflow.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "siliconflow",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|siliconflow"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "Qwen/Qwen2-7B-Instruct",
          "dataProxy": "llm|silliconflow_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_lm_studio",
      "title": "LM Studio",
      "description": "Chat with LM Studio, learn more : [lmstudio.ai](https://lmstudio.ai/)",
      "icon": "lm_studio.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "lm_studio",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|lm_studio"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "title": "Model Name",
          "dataProxy": "llm|lmstudio_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_openrouter",
      "title": "OpenRouter",
      "description": "Chat with OpenRouter, learn more: [openrouter.ai](https://openrouter.ai/)",
      "icon": "openrouter.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "openrouter",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|openrouter"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "anthropic/claude-3.5-sonnet",
          "dataProxy": "llm|openrouter_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "google_search",
          "description": "Grounding with Google Search connects the Gemini model to real-time web content and works with all available languages. This allows Gemini to provide more accurate answers and cite verifiable sources beyond its knowledge cutoff.",
          "type": "checkbox",
          "required": false,
          "title": "Google Search Tool",
          "default": false,
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash-lite",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "url_context",
          "description": "The URL context tool lets you provide additional context to the models in the form of URLs. By including URLs in your request, the model will access the content from those pages (as long as it's not a URL type listed in the limitations section) to inform and enhance its response.",
          "type": "checkbox",
          "required": false,
          "title": "URL Context Tool",
          "default": false,
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash-lite",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "chat_fireworks",
      "title": "Fireworks",
      "description": "Chat with Fireworks, learn more : [fireworks.ai](https://fireworks.ai), get all models from [fireworks models](https://fireworks.ai/models/fireworks/mixtral-8x7b-instruct)",
      "icon": "fireworks.jpg",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "fireworks",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|fireworks"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "accounts/fireworks/models/deepseek-v3-0324",
          "dataProxy": "llm|fireworks_models",
          "dataCustomTitle": "Add Custom Model"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_cloudflare",
      "title": "Cloudflare Workers AI",
      "description": "Chat with Cloudflare Workers AI, learn more : [workers-ai](https://developers.cloudflare.com/workers-ai/)",
      "icon": "cloudflare.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "cloudflare",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|cloudflare"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "@cf/meta/llama-2-7b-chat-int8",
          "dataProxy": "llm|cloudflare_models"
        }
      ]
    },
    {
      "name": "chat_together_ai",
      "title": "Together AI",
      "description": "Chat with together.ai, learn more : [together.ai](https://docs.together.ai/docs/quickstart), get chat models from [together models](https://docs.together.ai/docs/inference-models)",
      "icon": "together.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "together_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|together_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "meta-llama/Llama-3.2-3B-Instruct-Turbo",
          "dataProxy": "llm|together_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_poe",
      "title": "Poe",
      "description": "Chat with Poe, learn more : [poe.com](https://poe.com)",
      "icon": "poe.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "poe",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|poe"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "claude-3.5-sonnet",
          "dataProxy": "llm|poe_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_moonshot_ai",
      "title": "MoonShot AI",
      "description": "Chat with moonshot ai, learn more : [moonshot.cn](https://www.moonshot.cn/)",
      "icon": "moonshot.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "moonshot_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|moonshot_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "kimi-k2-0711-preview",
          "dataProxy": "llm|moonshot_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_dashscope",
      "title": "DashScope",
      "description": "Chat with 阿里云百炼 , learn more : [阿里云百炼](https://bailian.console.aliyun.com/#/home)",
      "icon": "dashscope.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "dashscope",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|dashscope"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "qwen-max-latest",
          "dataProxy": "llm|dashscope_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_sambanova",
      "title": "Sambanova AI",
      "description": "Chat with sambanova.ai, learn more : [sambanova.ai](https://cloud.sambanova.ai/pricing)",
      "icon": "sambanova.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "sambanova",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|sambanova"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "Meta-Llama-3.2-3B-Instruct",
          "dataProxy": "llm|sambanova_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_cerebras",
      "title": "Cerebras AI",
      "description": "Chat with cerebras.ai, learn more : [cerebras.ai](https://cerebras.ai/)",
      "icon": "cerebras.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "cerebras",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|cerebras"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama3.1-8b",
          "dataProxy": "llm|cerebras_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_arli",
      "title": "Arli AI",
      "description": "Chat with arli.ai, learn more : [arliai.com](https://arliai.com/)",
      "icon": "arli.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "arli",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|arli"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "(TRIAL) Llama-3.1-70B-ArliAI-RPMax-v1.1",
          "dataProxy": "llm|arli_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_straico",
      "title": "Straico AI",
      "description": "Chat with straico.com, learn more : [straico.com](https://straico.com/)",
      "icon": "straico.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "straico",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|straico"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "openai/gpt-4o-mini",
          "dataProxy": "llm|straico_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "302_ai",
      "title": "302.AI",
      "description": "Chat with 302.ai, learn more : [302.ai](https://share.302.ai/jMFReu)",
      "icon": "302.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "302_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|302_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "qwen2.5-3b-instruct",
          "dataProxy": "llm|302_ai_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "ollama_models",
      "title": "Ollama Models",
      "commandType": "function_command",
      "description": "get ollama model list",
      "icon": "ollama.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "ollama",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|ollama"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "openrouter_models",
      "commandType": "function_command",
      "title": "openrouter Models",
      "description": "get openrouter model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "perplexity_models",
      "targetCommand": "llm|fetch_models",
      "commandType": "function_command",
      "title": "perplexity Models",
      "description": "get perplexity model list",
      "icon": "perplexity.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "title": "Url",
          "description": "perplexity api url",
          "type": "textfield",
          "required": false,
          "default": "https://file.enconvo.com/modles/perplexity.json"
        }
      ]
    },
    {
      "name": "groq_models",
      "title": "groq Models",
      "commandType": "function_command",
      "description": "get groq model list",
      "icon": "groq.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "groq",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|groq"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "x_ai_models",
      "title": "X.AI Models",
      "description": "get X.AI model list",
      "commandType": "function_command",
      "icon": "x_ai.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "x_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|x_ai"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "moonshot_models",
      "title": "Moonshot Models",
      "description": "get Moonshot model list",
      "commandType": "function_command",
      "icon": "moonshot.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "moonshot_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|moonshot_ai"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "straico_models",
      "title": "Straico Models",
      "description": "get Straico model list",
      "commandType": "function_command",
      "icon": "straico.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "straico",
          "defaultProxy": "llm|chat_straico",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|straico"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "type",
          "description": "How to get api key? [🔑here](https://straico.com/)",
          "type": "textfield",
          "required": false,
          "title": "Type",
          "default": "chat"
        }
      ]
    },
    {
      "name": "gemini_models",
      "title": "Gemini Models",
      "description": "get Gemini model list",
      "commandType": "function_command",
      "icon": "gemini.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "gemini",
          "extensionType": "credentials-provider",
          "defaultProxy": "llm|chat_google",
          "extensionFilter": {
            "targetCommands": [
              "credentials|gemini"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "deepseek_models",
      "title": "DeepSeek Models",
      "description": "get DeepSeek model list",
      "commandType": "function_command",
      "icon": "deepseek.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": []
    },
    {
      "name": "dashscope_models",
      "title": "DashScope Models",
      "description": "get DashScope model list",
      "commandType": "function_command",
      "icon": "dashscope.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "apiKey",
          "description": "How to get api key? [🔑here](https://bailian.console.aliyun.com/#/home)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "credentials|dashscope",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "DashScope api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://dashscope.aliyuncs.com/compatible-mode/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "sambanova_models",
      "title": "Sambanova Models",
      "description": "get Sambanova model list",
      "commandType": "function_command",
      "icon": "sambanova.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "sambanova",
          "defaultProxy": "llm|chat_sambanova",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|sambanova"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "cerebras_models",
      "title": "Cerebras Models",
      "description": "get Cerebras model list",
      "commandType": "function_command",
      "icon": "cerebras.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "cerebras",
          "defaultProxy": "llm|chat_cerebras",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|cerebras"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "mistral_models",
      "title": "Mistral Models",
      "description": "get Mistral model list",
      "commandType": "function_command",
      "icon": "mistral.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "apiKey",
          "description": "How to get api key? [🔑here](https://console.mistral.ai/api-keys/)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "credentials|mistral",
          "placeholder": "Mistral AI Api Key"
        },
        {
          "name": "baseUrl",
          "description": "Mixtral api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "https://api.mistral.ai/v1",
          "defaultProxy": "credentials|mistral",
          "placeholder": "Mixtral Api Base Url"
        }
      ]
    },
    {
      "name": "poe_models",
      "title": "Poe Models",
      "description": "get Poe model list",
      "commandType": "function_command",
      "icon": "poe.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "poe",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|poe"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "together_models",
      "targetCommand": "llm|fetch_models",
      "title": "together Models",
      "commandType": "function_command",
      "description": "get together model list",
      "icon": "together.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "title": "Url",
          "name": "url",
          "description": "together api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/together/v1/models"
        }
      ]
    },
    {
      "name": "fireworks_models",
      "title": "fireworks Models",
      "commandType": "function_command",
      "description": "get fireworks model list",
      "icon": "fireworks.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "fireworks",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|fireworks"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "302_ai_models",
      "title": "302 AI Models",
      "commandType": "function_command",
      "description": "get 302 AI model list",
      "icon": "302.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "302_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|302_ai"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "fetch_models",
      "title": "fetch Models",
      "description": "get models list",
      "icon": "together.png",
      "commandType": "function_command",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "enconvo_models",
      "targetCommand": "llm|fetch_models",
      "title": "enconvo Models",
      "commandType": "function_command",
      "description": "get enconvo model list",
      "icon": "enconvo.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "enconvo api url",
          "type": "textfield",
          "required": false,
          "default": "https://file.enconvo.com/modles/enconvo.json"
        }
      ]
    },
    {
      "name": "qwen_models",
      "title": "qwen Models",
      "description": "get qwen model list",
      "icon": "qwen.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "qwen",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|qwen"
            ]
          },
          "title": "Credential Provider",
          "defaultProxy": "llm|chat_qwen"
        }
      ]
    },
    {
      "name": "vercel_ai_gateway_models",
      "title": "vercel_ai_gateway Models",
      "description": "get vercel_ai_gateway model list",
      "icon": "vercel.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "vercel_ai_gateway",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|vercel_ai_gateway"
            ]
          },
          "title": "Credential Provider",
          "defaultProxy": "llm|chat_vercel_ai_gateway"
        }
      ]
    },
    {
      "name": "openai_models",
      "title": "openai Models",
      "description": "get openai model list",
      "icon": "openai.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "open_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|open_ai"
            ]
          },
          "title": "Credential Provider",
          "defaultProxy": "llm|chat_open_ai"
        }
      ]
    },
    {
      "name": "lmstudio_models",
      "title": "lmstudio Models",
      "description": "get lmstudio model list",
      "icon": "lmstudio.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "targetCommand": "llm|openai_models",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "lm_studio",
          "defaultProxy": "llm|chat_lm_studio",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|lm_studio"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "azureai_models",
      "title": "azureai Models",
      "targetCommand": "llm|fetch_models",
      "description": "get azureai model list",
      "icon": "azureai.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "openai api url",
          "type": "textfield",
          "required": false,
          "default": "https://file.enconvo.com/modles/openai.json"
        }
      ]
    },
    {
      "name": "cloudflare_models",
      "title": "cloudflare Models",
      "targetCommand": "llm|fetch_models",
      "description": "get cloudflare model list",
      "icon": "cloudflare.png",
      "commandType": "function_command",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "cloudflare api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/cloudflare/v1/models"
        }
      ]
    },
    {
      "name": "cohere_models",
      "title": "cohere Models",
      "targetCommand": "llm|fetch_models",
      "description": "get cohere model list",
      "icon": "cohere.jpg",
      "commandType": "function_command",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "title": "Url",
          "description": "cohere api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/cohere/v1/models"
        }
      ]
    },
    {
      "name": "silliconflow_models",
      "title": "silliconflow_models",
      "targetCommand": "llm|fetch_models",
      "description": "get silliconflow model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "silliconflow api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/siliconflow/v1/models"
        }
      ]
    },
    {
      "name": "chat_aimagicx",
      "title": "AIMagicX",
      "description": "Chat with AIMagicX, learn more : [aimagicx.com](https://aimagicx.com/)",
      "icon": "aimagicx.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "aimagicx",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|aimagicx"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "4o-mini",
          "dataProxy": "llm|aimagicx_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 0.7,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 0.7
            },
            {
              "title": "high",
              "value": 1
            }
          ]
        },
        {
          "name": "maxTokens",
          "description": "The maximum number of tokens to generate in the completion.",
          "type": "textfield",
          "required": false,
          "title": "Max Tokens",
          "default": "4096"
        }
      ]
    },
    {
      "name": "aimagicx_models",
      "title": "aimagicx_models",
      "targetCommand": "llm|aimagicx_models",
      "description": "get AIMagicX model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "aimagicx",
          "extensionType": "credentials-provider",
          "defaultProxy": "llm|chat_aimagicx",
          "extensionFilter": {
            "targetCommands": [
              "credentials|aimagicx"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "chat_1minai",
      "title": "1min AI",
      "description": "Chat with 1min AI, learn more : [1min.ai](https://1min.ai/)",
      "icon": "1minai.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "1minai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|1minai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gpt-4o-mini",
          "dataProxy": "llm|1minai_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 0.7,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 0.7
            },
            {
              "title": "high",
              "value": 1
            }
          ]
        },
        {
          "name": "maxTokens",
          "description": "The maximum number of tokens to generate in the completion.",
          "type": "textfield",
          "required": false,
          "title": "Max Tokens",
          "default": "4096"
        }
      ]
    },
    {
      "name": "1minai_models",
      "title": "1minai_models",
      "targetCommand": "llm|1minai_models",
      "description": "get 1min AI model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "1minai",
          "extensionType": "credentials-provider",
          "defaultProxy": "llm|chat_1minai",
          "extensionFilter": {
            "targetCommands": [
              "credentials|1minai"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "arli_models",
      "title": "arli_models",
      "targetCommand": "llm|fetch_models",
      "description": "get arli model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "arli",
          "extensionType": "credentials-provider",
          "defaultProxy": "llm|chat_arli",
          "extensionFilter": {
            "targetCommands": [
              "credentials|arli"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "chat_vercel_ai_gateway",
      "title": "Vercel AI Gateway",
      "description": "Chat using Vercel AI Gateway with support for multiple AI providers, learn more: [vercel.com/ai-gateway](https://vercel.com/ai-gateway)",
      "icon": "vercel.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "vercel_ai_gateway",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|vercel_ai_gateway"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion. Format: provider/model (e.g., openai/gpt-5, anthropic/claude-sonnet-4)",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "openai/gpt-5",
          "dataProxy": "llm|vercel_ai_gateway_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "google_search",
          "description": "Grounding with Google Search connects the Gemini model to real-time web content and works with all available languages. This allows Gemini to provide more accurate answers and cite verifiable sources beyond its knowledge cutoff.",
          "type": "checkbox",
          "required": false,
          "title": "Google Search Tool",
          "default": false,
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash-lite",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "url_context",
          "description": "The URL context tool lets you provide additional context to the models in the form of URLs. By including URLs in your request, the model will access the content from those pages (as long as it's not a URL type listed in the limitations section) to inform and enhance its response.",
          "type": "checkbox",
          "required": false,
          "title": "URL Context Tool",
          "default": false,
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash-lite",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "claude_thinking",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Extended thinking",
          "default": "disabled",
          "data": [
            {
              "title": "Disabled",
              "value": "disabled",
              "description": "Model does not think"
            },
            {
              "title": "Minimal",
              "value": "1024",
              "description": "Thinking budget tokens: 1024"
            },
            {
              "title": "Low",
              "value": "2048",
              "description": "Thinking budget tokens: 2048"
            },
            {
              "title": "Medium",
              "value": "5120",
              "description": "Thinking budget tokens: 5120"
            },
            {
              "title": "High",
              "value": "10240",
              "description": "Thinking budget tokens: 10240"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "anthropic/claude-opus-4",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "anthropic/claude-opus-4",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "anthropic/claude-sonnet-4",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "anthropic/claude-3-7-sonnet",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "low",
          "data": [
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "openai/o1-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o1",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o1-pro",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o3-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o3",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o4-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "deepseek-ai/DeepSeek-R1",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai-oss/gpt-oss-20b",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai-oss/gpt-oss-120b",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "reasoning_effort_new",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "minimal",
          "data": [
            {
              "title": "minimal",
              "value": "minimal",
              "description": "Favors speed and economical token usage with minimal reasoning"
            },
            {
              "title": "low",
              "value": "low",
              "description": "Favors speed and economical token usage with basic reasoning"
            },
            {
              "title": "medium",
              "value": "medium",
              "description": "Balance between speed and reasoning accuracy (default)"
            },
            {
              "title": "high",
              "value": "high",
              "description": "Favors more complete reasoning with higher token usage"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "openai/gpt-5-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/gpt-5",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/gpt-5-nano",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "gemini_thinking_pro",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Gemini thinking",
          "default": "auto",
          "data": [
            {
              "title": "Auto",
              "value": "auto",
              "description": "Model decides when and how much to think"
            },
            {
              "title": "Minimal",
              "value": "128",
              "description": "Thinking budgets: 128"
            },
            {
              "title": "Low",
              "value": "1024",
              "description": "Thinking budgets: 1024"
            },
            {
              "title": "Medium",
              "value": "10000",
              "description": "Thinking budgets: 10000"
            },
            {
              "title": "High",
              "value": "30000",
              "description": "Thinking budgets: 30000"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-pro",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "gemini_thinking",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Gemini thinking",
          "default": "disabled",
          "data": [
            {
              "title": "Disabled",
              "value": "disabled",
              "description": "Model does not think"
            },
            {
              "title": "Auto",
              "value": "auto",
              "description": "Model decides when and how much to think"
            },
            {
              "title": "Minimal",
              "value": "512",
              "description": "Thinking budgets: 512"
            },
            {
              "title": "Low",
              "value": "1024",
              "description": "Thinking budgets: 1024"
            },
            {
              "title": "Medium",
              "value": "10000",
              "description": "Thinking budgets: 10000"
            },
            {
              "title": "High",
              "value": "20000",
              "description": "Thinking budgets: 20000"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "google/gemini-2.5-flash-lite",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    }
  ],
  "dependencies": {
    "@ai-sdk/gateway": "^1.0.21",
    "@ai-sdk/openai": "^2.0.15",
    "@anthropic-ai/sdk": "^0.60.0",
    "@enconvo/api": "link:/Users/ysnows/Documents/Project/enconvo.nodejs/enconvo_api",
    "@google/genai": "^1.14.0",
    "@langchain/cloudflare": "^0.1.0",
    "@langchain/cohere": "^0.3.1",
    "@langchain/community": "^0.3.19",
    "@langchain/core": "^0.3.23",
    "@lmnr-ai/lmnr": "^0.4.42",
    "@types/wav": "^1.0.4",
    "ai": "^5.0.15",
    "axios": "^1.7.9",
    "form-data": "^4.0.1",
    "google-auth-library": "^10.3.0",
    "langchain": "^0.3.7",
    "langsmith": "^0.3.28",
    "mime": "^4.0.6",
    "ollama": "^0.5.17",
    "open": "^10.2.0",
    "openai": "^5.20.2",
    "wav": "^1.0.2",
    "zod": "^4.0.17"
  },
  "devDependencies": {
    "@types/node": "^22.10.2",
    "eslint": "^9.17.0",
    "prettier": "^3.4.2",
    "tsup": "^8.3.5",
    "typescript": "^5.7.2"
  },
  "scripts": {
    "lint": "eslint src",
    "lint:fix": "npm run lint --fix",
    "format": "prettier --write \"**/*.ts\"",
    "format:check": "prettier --list-different \"**/*.ts\"",
    "build": "enconvo",
    "dev": "enconvo --dev"
  },
  "minAppVersion": "1.8.8"
}