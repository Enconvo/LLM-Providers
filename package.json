{
  "$schema": "https://enconvo.com/schemas/extension.json",
  "name": "llm",
  "version": "1.3.5",
  "description": "Large Language Models (LLM) services providers",
  "title": "LLM",
  "icon": "icon.png",
  "author": "ysnows",
  "license": "MIT",
  "categories": [
    "Large Language Model"
  ],
  "group": "llm",
  "type": "module",
  "commands": [
    {
      "name": "enconvo_ai",
      "title": "EnConvoAI",
      "description": "Chat using EnconvoAI which provide LLM service",
      "icon": "enconvo.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "md:The model to generate the completion. Every call will consume points, and the specific consumption details are as followsï¼š\n\n  **OpenAI:** \n\n | Model Name | Points |\n | --- | --- |\n | gpt-35-turbo | 50 |\n | gpt4 | 875 |\n\n\n | gpt4 | 875 |\n\n\n | gpt4-vision | 1250 |\n\n\n | Claude 3 Sonnet | 1250 |\n\n\n | Claude 3 Opus | 6000 |\n\n",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "openai/gpt-35-turbo",
          "data": [
            {
              "title": "OpenAI: GPT-3.5 Turbo 16k (50 points)",
              "value": "openai/gpt-35-turbo"
            },
            {
              "title": "OpenAI: GPT-4 Turbo 128k (875 points)",
              "value": "openai/gpt4"
            },
            {
              "title": "OpenAI: GPT-4 Vision 128k (1250 points)",
              "value": "openai/gpt4-vision"
            },
            {
              "title": "Mistral Medium (900 points)",
              "value": "mistralai/mistral-medium"
            },
            {
              "title": "Mistral Large (1200 points)",
              "value": "mistralai/mistral-large"
            },
            {
              "title": "Nous: Capybara 7B (5 points)",
              "value": "nousresearch/nous-capybara-7b:free"
            },
            {
              "title": "Mistral 7B Instruct (5 points)",
              "value": "mistralai/mistral-7b-instruct:free"
            },
            {
              "title": "MythoMist 7B (5 points)",
              "value": "gryphe/mythomist-7b:free"
            },
            {
              "title": "Toppy M 7B (5 points)",
              "value": "undi95/toppy-m-7b:free"
            },
            {
              "title": "Cinematika 7B (alpha) (5 points)",
              "value": "openrouter/cinematika-7b:free"
            },
            {
              "title": "Google: Gemma 7B (5 points)",
              "value": "google/gemma-7b-it:free"
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_open_ai",
      "title": "OpenAI",
      "description": "Chat with OpenAI , learn more : [openai developers](https://platform.openai.com/docs/overview)",
      "icon": "openai.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gpt-3.5-turbo-0125",
          "data": [
            {
              "title": "gpt-3.5-turbo-0125",
              "value": "gpt-3.5-turbo-0125"
            },
            {
              "title": "gpt-4-0125-preview",
              "value": "gpt-4-0125-preview"
            },
            {
              "title": "gpt-4-vision-preview",
              "value": "gpt-4-vision-preview"
            },
            {
              "title": "gpt-3.5-turbo",
              "value": "gpt-3.5-turbo"
            },
            {
              "title": "gpt-3.5-turbo-1106",
              "value": "gpt-3.5-turbo-1106"
            },
            {
              "title": "gpt-3.5-turbo-0613",
              "value": "gpt-3.5-turbo-0613"
            },
            {
              "title": "gpt-3.5-turbo-16k",
              "value": "gpt-3.5-turbo-16k"
            },
            {
              "title": "gpt-3.5-turbo-16k-0613",
              "value": "gpt-3.5-turbo-16k-0613"
            },
            {
              "title": "gpt-4-turbo-preview",
              "value": "gpt-4-turbo-preview"
            },
            {
              "title": "gpt-4",
              "value": "gpt-4"
            },
            {
              "title": "gpt-4-0613",
              "value": "gpt-4-0613"
            },
            {
              "title": "gpt-4-32k",
              "value": "gpt-4-32k"
            },
            {
              "title": "gpt-4-32k-0613",
              "value": "gpt-4-32k-0613"
            },
            {
              "title": "gpt-4-1106-preview",
              "value": "gpt-4-1106-preview"
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "password",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.openai.com/v1",
          "placeholder": "OpenAI Api Base Url"
        },
        {
          "name": "customHeaders",
          "description": "Custom Headers, In Json Format",
          "type": "textfield",
          "required": false,
          "title": "Custom Headers",
          "default": "{}",
          "placeholder": "{}"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "azure_openai",
      "title": "Azure OpenAI",
      "description": "Azure OpenAI , learn more : [azure openai](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)",
      "icon": "azure.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "azureOpenAIApiDeploymentName",
          "description": "Azure OpenAI api deployment name",
          "type": "textfield",
          "required": false,
          "title": "Azure OpenAI Api Deployment Name",
          "default": ""
        },
        {
          "name": "azureOpenAIApiInstanceName",
          "description": "Azure OpenAI api instance name",
          "type": "textfield",
          "required": false,
          "title": "Azure OpenAI Api Instance Name",
          "default": ""
        },
        {
          "name": "azureOpenAIApiVersion",
          "description": "Azure OpenAI api version",
          "type": "textfield",
          "required": false,
          "title": "Azure OpenAI Api Version",
          "default": "2024-02-15-preview"
        },
        {
          "name": "azureOpenAIApiKey",
          "description": "Azure OpenAI api key",
          "type": "password",
          "required": false,
          "title": "Azure OpenAI Api Key",
          "default": "",
          "placeholder": "Azure OpenAI Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_anthropic",
      "title": "Anthropic Claude",
      "description": "Chat with Anthropic Claude, learn more : [anthropic.com](https://www.anthropic.com/)",
      "mode": "llm",
      "icon": "anthropic.png",
      "preferences": [
        {
          "name": "modelName",
          "description": "md: The model to generate the completion. \n|               | Claude 3 Opus | Claude 3 Sonnet | Claude 3 Haiku |\n|---------------|--------------|----------------|---------------|\n| Context window| 200K*        | 200K*          | 200K*         |\n| Cost (Input / Output per MTok^)| $15.00/$75.00 | $3.00/$15.00   | $0.25/$1.25    | $8.00/$24.00  |",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "claude-3-opus-20240229",
          "data": [
            {
              "title": "Claude 3 Opus 200k",
              "value": "claude-3-opus-20240229"
            },
            {
              "title": "Claude 3 Sonnet 200k",
              "value": "claude-3-sonnet-20240229"
            },
            {
              "title": "Claude 3 Haiku 200k",
              "value": "claude-3-haiku-20240307"
            },
            {
              "title": "claude-2 100k",
              "value": "claude-2"
            },
            {
              "title": "claude-instant-1 100k",
              "value": "claude-instant-1"
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "anthropicApiKey",
          "description": "Anthropic api key",
          "type": "password",
          "required": false,
          "title": "Claude Api Key",
          "default": "",
          "placeholder": "Claude Api Key"
        },
        {
          "name": "anthropicApiUrl",
          "description": "Anthropic api base url",
          "type": "textfield",
          "required": false,
          "title": "Anthropic Api Base Url",
          "default": "https://api.anthropic.com",
          "placeholder": "Anthropic Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "ollama_models",
      "title": "Ollama Models",
      "description": "get ollama model list",
      "icon": "ollama.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "llm",
          "description": "The model used to generate responses",
          "type": "extension",
          "required": false,
          "default": "default",
          "title": "LLM Model"
        }
      ]
    },
    {
      "name": "chat_lm_studio",
      "title": "LM Studio",
      "description": "Chat with LM Studio, learn more : [lmstudio.ai](https://lmstudio.ai/)",
      "icon": "lm_studio.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "baseUrl",
          "description": "LM Studio api base url",
          "type": "textfield",
          "required": false,
          "title": "LM Studio Api Base Url",
          "default": "http://127.0.0.1:1234/v1",
          "placeholder": "LM Studio Api Base Url"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_ollama",
      "title": "ChatOllama",
      "description": "Chat with Ollama, learn more : [ollama.ai](https://ollama.ai/)",
      "icon": "ollama.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "model",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama2:latest",
          "data": [],
          "dataProxy": "llm|ollama_models"
        },
        {
          "name": "baseUrl",
          "description": "Ollama api base url",
          "type": "textfield",
          "required": false,
          "title": "Ollama Api Base Url",
          "default": "http://127.0.0.1:11434",
          "placeholder": "Ollama Api Base Url"
        }
      ]
    },
    {
      "name": "chat_openrouter",
      "title": "OpenRouter",
      "description": "Chat with OpenRouter, learn more: [openrouter.ai](https://openrouter.ai/)",
      "icon": "openrouter.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "openai/gpt-3.5-turbo",
          "data": [],
          "dataProxy": "llm|openrouter_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenRouter api key",
          "type": "password",
          "required": false,
          "title": "OpenRouter Api Key",
          "default": "",
          "placeholder": "OpenRouter Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenRouter api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenRouter Api Base Url",
          "default": "https://openrouter.ai/api/v1",
          "placeholder": "OpenRouter Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "openrouter_models",
      "title": "openrouter Models",
      "description": "get openrouter model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "chat_perplexity",
      "title": "Perplexity",
      "description": "Chat with Perplexity, learn more: [perplexity.ai](https://www.perplexity.ai/settings/api)",
      "icon": "perplexity.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "pplx-7b-online",
          "data": [],
          "dataProxy": "llm|perplexity_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "Perplexity api key",
          "type": "password",
          "required": false,
          "title": "Perplexity Api Key",
          "default": "",
          "placeholder": "Perplexity Api Key"
        },
        {
          "name": "frequencyPenalty",
          "description": "frequencyPenalty",
          "type": "textfield",
          "required": false,
          "title": "frequencyPenalty",
          "default": "1"
        },
        {
          "name": "baseUrl",
          "description": "Perplexity api base url",
          "type": "textfield",
          "required": false,
          "title": "Perplexity Api Base Url",
          "default": "https://api.perplexity.ai",
          "placeholder": "Perplexity Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "perplexity_models",
      "title": "perplexity Models",
      "description": "get perplexity model list",
      "icon": "perplexity.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "chat_mistral",
      "title": "Mistral AI",
      "description": "Chat with Mistral AI, learn more : [mistral.ai](https://mistral.ai/)",
      "icon": "mistral.jpg",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "md:The model to generate the completion. \n\n ## Pricing and rate limits\n\n**Pay-as-you-go**\n\nThe prices listed below are exclusive of VAT.\n\n### Chat Completions API\n\n| Model           | Endpoint                | Input (USD)         | Output (USD)         |\n|-----------------|-------------------------|---------------------|----------------------|\n| Mistral 7B      | open-mistral-7b         | 0.25$ / 1M tokens   | 0.25$ / 1M tokens    |\n| Mixtral 8x7B    | open-mixtral-8x7b       | 0.7$ / 1M tokens    | 0.7$ / 1M tokens     |\n| Mistral Small   | mistral-small-latest     | 2$ / 1M tokens     | 6$ / 1M tokens       |\n| Mistral Medium  | mistral-medium-latest    | 2.7$ / 1M tokens   | 8.1$ / 1M tokens     |\n| Mistral Large   | mistral-large-latest     | 8$ / 1M tokens     | 24$ / 1M tokens      |\n",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "mistral-tiny",
          "data": [
            {
              "title": "Mistral 7B",
              "value": "open-mistral-7b"
            },
            {
              "title": "Mixtral 8x7B",
              "value": "open-mixtral-8x7b"
            },
            {
              "title": "Mistral Small",
              "value": "mistral-small-latest"
            },
            {
              "title": "Mistral Medium",
              "value": "mistral-medium-latest"
            },
            {
              "title": "Mistral Large",
              "value": "mistral-large-latest"
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "Mistral AI api key",
          "type": "password",
          "required": false,
          "title": "Mistral AI Api Key",
          "default": "",
          "placeholder": "Mistral AI Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_google",
      "title": "Google Gemini AI",
      "description": "Chat with Google Gemini AI, learn more : [Google AI](https://ai.google.dev/)",
      "icon": "google.jpg",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemini-pro",
          "data": [
            {
              "title": "Gemini 1.0 Pro",
              "value": "gemini-pro"
            },
            {
              "title": "Gemini 1.5 Pro",
              "value": "gemini-1.5-pro"
            },
            {
              "title": "gemini-pro-vision",
              "value": "gemini-pro-vision"
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "Google AI api key",
          "type": "password",
          "required": false,
          "title": "Google AI Api Key",
          "default": "",
          "placeholder": "Google AI Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_fireworks",
      "title": "Fireworks",
      "description": "Chat with Fireworks, learn more : [fireworks.ai](https://fireworks.ai), get all models from [fireworks models](https://fireworks.ai/models/fireworks/mixtral-8x7b-instruct)",
      "icon": "fireworks.jpg",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "textfield",
          "required": false,
          "title": "Model Name",
          "default": "accounts/fireworks/models/mixtral-8x7b-instruct"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "password",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.fireworks.ai/inference/v1",
          "placeholder": "OpenAI Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_cloudflare",
      "title": "Cloudflare Workers AI",
      "description": "Chat with Cloudflare Workers AI, learn more : [workers-ai](https://developers.cloudflare.com/workers-ai/)",
      "icon": "cloudflare.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "@cf/meta/llama-2-7b-chat-int8",
          "data": [],
          "dataProxy": "llm|cloudflare_models"
        },
        {
          "name": "account_id",
          "description": "Cloudflare AccountID",
          "type": "textfield",
          "required": false,
          "title": "Cloudflare AccountID",
          "default": "",
          "placeholder": "Cloudflare AccountID"
        },
        {
          "name": "apiKey",
          "description": "Cloudflare api Token",
          "type": "password",
          "required": false,
          "title": "Cloudflare Api Token",
          "default": "",
          "placeholder": "Cloudflare Api Token"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "cloudflare_models",
      "title": "cloudflare Models",
      "description": "get cloudflare model list",
      "icon": "cloudflare.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "chat_together_ai",
      "title": "Together AI",
      "description": "Chat with together.ai, learn more : [together.ai](https://docs.together.ai/docs/quickstart), get chat models from [together models](https://docs.together.ai/docs/inference-models)",
      "icon": "together.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "textfield",
          "required": false,
          "title": "Model Name",
          "default": "mistralai/Mixtral-8x7B-Instruct-v0.1"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "TOGETHER_API_KEY",
          "type": "password",
          "required": false,
          "title": "TOGETHER_API_KEY",
          "default": "",
          "placeholder": "Together Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.together.xyz/v1",
          "placeholder": "OpenAI Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_moonshot_ai",
      "title": "MoonShot AI",
      "description": "Chat with moonshot ai, learn more : [moonshot.cn](https://www.moonshot.cn/)",
      "icon": "moonshot.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "moonshot-v1-8k",
          "data": [
            {
              "title": "moonshot-v1-8k",
              "value": "moonshot-v1-8k"
            },
            {
              "title": "moonshot-v1-32k",
              "value": "moonshot-v1-32k"
            },
            {
              "title": "moonshot-v1-128k",
              "value": "moonshot-v1-128k"
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "MOONSHOT_API_KEY",
          "type": "password",
          "required": false,
          "title": "MOONSHOT_API_KEY",
          "default": "",
          "placeholder": "MoonShot Api Key"
        },
        {
          "name": "baseUrl",
          "description": "MoonShot api base url",
          "type": "textfield",
          "required": false,
          "title": "MoonShot Api Base Url",
          "default": "https://api.moonshot.cn/v1",
          "placeholder": "MoonShot Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_groq",
      "title": "Groq AI",
      "description": "Chat with Groq ai, learn more : [groq.com/](https://groq.com/)",
      "icon": "groq.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama2-70b-4096",
          "data": [
            {
              "title": "LLaMA2-70b-chat",
              "value": "llama2-70b-4096"
            },
            {
              "title": "Mixtral-8x7b-Instruct-v0.1",
              "value": "mixtral-8x7b-32768"
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "Groq_API_KEY",
          "type": "password",
          "required": false,
          "title": "Groq_API_KEY",
          "default": "",
          "placeholder": "Groq Api Key"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    }
  ],
  "dependencies": {
    "@enconvo/api": "^0.1.2",
    "@google/generative-ai": "^0.1.3",
    "@langchain/anthropic": "^0.1.2",
    "@langchain/community": "^0.0.13",
    "@langchain/core": "^0.1.52",
    "@langchain/google-genai": "^0.0.5",
    "@langchain/groq": "^0.0.1",
    "@langchain/mistralai": "^0.0.5",
    "langchain": "^0.1.30"
  },
  "devDependencies": {
    "@types/node": "^18.17.14",
    "eslint": "^8.33.0",
    "prettier": "^2.8.3",
    "tsup": "^7.2.0",
    "typescript": "^5.0.4"
  },
  "scripts": {
    "lint": "eslint src",
    "lint:fix": "npm run lint --fix",
    "format": "prettier --write \"**/*.ts\"",
    "format:check": "prettier --list-different \"**/*.ts\"",
    "build": "enconvo",
    "dev": "enconvo --dev"
  }
}