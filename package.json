{
  "$schema": "https://enconvo.com/schemas/extension.json",
  "name": "llm",
  "version": "1.3.78",
  "description": "Large Language Models (LLM) services providers",
  "title": "LLM Providers",
  "icon": "icon.png",
  "author": "ysnows",
  "license": "MIT",
  "categories": [
    "Large Language Model"
  ],
  "group": "llm",
  "type": "module",
  "commands": [
    {
      "name": "enconvo_ai",
      "title": "EnConvoAI",
      "description": "Chat using EnconvoAI which provide LLM service",
      "icon": "enconvo.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "md:The model to generate the completion. Every call will consume points, and the specific consumption details are as follows\uff1a\n\n  **OpenAI:** \n\n | Model Name | Points |\n | --- | --- |\n | gpt-35-turbo | 50 |\n | gpt4 | 875 |\n\n\n | gpt4 | 875 |\n\n\n | gpt4-vision | 1250 |\n\n\n | Claude 3 Sonnet | 1250 |\n\n\n | Claude 3 Opus | 6000 |\n\n",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "enconvoai/assistant",
          "data": [
            {
              "title": "EnconvoAI: Assistant 16k (50 points)",
              "value": "enconvoai/assistant",
              "context": 16000,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "OpenAI: GPT-3.5 Turbo 16k (50 points)",
              "value": "openai/gpt-3.5-turbo",
              "context": 16000,
              "toolUse": true
            },
            {
              "title": "OpenAI: GPT-4 Turbo 128k (875 points)",
              "value": "openai/gpt-4-turbo",
              "context": 128000,
              "visionEnable": true,
              "toolUse": true
            },
            {
              "title": "Claude 3 Opus 200k",
              "value": "anthropic/claude-3-opus-20240229",
              "context": 200000,
              "inputPrice": 0.15,
              "outputPrice": 0.75,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "Claude 3 Sonnet 200k",
              "value": "anthropic/claude-3-sonnet-20240229",
              "context": 200000,
              "inputPrice": 0.03,
              "outputPrice": 0.15,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "Claude 3 Haiku 200k",
              "value": "anthropic/claude-3-haiku-20240307",
              "inputPrice": 0.00025,
              "outputPrice": 0.00125,
              "context": 200000,
              "toolUse": true,
              "visionEnable": true
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "hidden",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "default",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "anthropicApiKey",
          "description": "OpenAI api key",
          "type": "hidden",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "default",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api key",
          "type": "hidden",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "https://api.enconvo.com/v1/",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "anthropicApiUrl",
          "description": "OpenAI api key",
          "type": "hidden",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "https://api.enconvo.com/",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_open_ai",
      "title": "OpenAI",
      "description": "Chat with OpenAI , learn more : [openai developers](https://platform.openai.com/docs/overview)",
      "icon": "openai.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gpt-3.5-turbo-0125",
          "data": [
            {
              "title": "gpt-3.5-turbo-0125",
              "value": "gpt-3.5-turbo-0125",
              "context": 16385,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "toolUse": true
            },
            {
              "title": "gpt-4-turbo",
              "value": "gpt-4-turbo",
              "context": 128000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "visionEnable": true,
              "toolUse": true
            },
            {
              "title": "gpt-4-turbo-2024-04-09",
              "value": "gpt-4-turbo-2024-04-09",
              "context": 128000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "visionEnable": true,
              "toolUse": true
            },
            {
              "title": "gpt-3.5-turbo",
              "value": "gpt-3.5-turbo",
              "context": 4096,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "toolUse": true
            },
            {
              "title": "gpt-3.5-turbo-1106",
              "value": "gpt-3.5-turbo-1106",
              "context": 16385,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "toolUse": true
            },
            {
              "title": "gpt-3.5-turbo-0613",
              "value": "gpt-3.5-turbo-0613",
              "context": 4096,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "toolUse": true
            },
            {
              "title": "gpt-3.5-turbo-16k",
              "value": "gpt-3.5-turbo-16k",
              "context": 16385,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "toolUse": true
            },
            {
              "title": "gpt-3.5-turbo-16k-0613",
              "value": "gpt-3.5-turbo-16k-0613",
              "context": 16385,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "toolUse": true
            },
            {
              "title": "gpt-4-turbo-preview",
              "value": "gpt-4-turbo-preview",
              "context": 128000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "toolUse": true
            },
            {
              "title": "gpt-4",
              "value": "gpt-4",
              "context": 8192,
              "inputPrice": 0.03,
              "outputPrice": 0.06,
              "toolUse": true
            },
            {
              "title": "gpt-4-0613",
              "value": "gpt-4-0613",
              "context": 8192,
              "inputPrice": 0.03,
              "outputPrice": 0.06,
              "toolUse": true
            },
            {
              "title": "gpt-4-32k",
              "value": "gpt-4-32k",
              "context": 32768,
              "inputPrice": 0.06,
              "outputPrice": 0.12,
              "toolUse": true
            },
            {
              "title": "gpt-4-32k-0613",
              "value": "gpt-4-32k-0613",
              "context": 32768,
              "inputPrice": 0.06,
              "outputPrice": 0.12,
              "toolUse": true
            },
            {
              "title": "gpt-4-1106-preview",
              "value": "gpt-4-1106-preview",
              "context": 128000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "toolUse": true
            },
            {
              "title": "gpt-4-vision-preview",
              "value": "gpt-4-vision-preview",
              "context": 128000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "visionEnable": true,
              "toolUse": true
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "password",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.openai.com/v1",
          "placeholder": "OpenAI Api Base Url"
        },
        {
          "name": "customHeaders",
          "description": "Custom Headers, In Json Format",
          "type": "textfield",
          "required": false,
          "title": "Custom Headers",
          "default": "{}",
          "placeholder": "{}"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "azure_openai",
      "title": "Azure OpenAI",
      "description": "Azure OpenAI , learn more : [azure openai](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)",
      "icon": "azure.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "azureOpenAIApiDeploymentName",
          "description": "Azure OpenAI api deployment name",
          "type": "textfield",
          "required": false,
          "title": "Azure OpenAI Api Deployment Name",
          "default": ""
        },
        {
          "name": "azureOpenAIApiInstanceName",
          "description": "Azure OpenAI api instance name",
          "type": "textfield",
          "required": false,
          "title": "Azure OpenAI Api Instance Name",
          "default": ""
        },
        {
          "name": "azureOpenAIApiVersion",
          "description": "Azure OpenAI api version",
          "type": "textfield",
          "required": false,
          "title": "Azure OpenAI Api Version",
          "default": "2024-02-15-preview"
        },
        {
          "name": "azureOpenAIApiKey",
          "description": "Azure OpenAI api key",
          "type": "password",
          "required": false,
          "title": "Azure OpenAI Api Key",
          "default": "",
          "placeholder": "Azure OpenAI Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_anthropic",
      "title": "Anthropic Claude",
      "description": "Chat with Anthropic Claude, learn more : [anthropic.com](https://www.anthropic.com/)",
      "mode": "llm",
      "icon": "anthropic.png",
      "preferences": [
        {
          "name": "modelName",
          "description": "md: The model to generate the completion. \n|               | Claude 3 Opus | Claude 3 Sonnet | Claude 3 Haiku |\n|---------------|--------------|----------------|---------------|\n| Context window| 200K*        | 200K*          | 200K*         |\n| Cost (Input / Output per MTok^)| $15.00/$75.00 | $3.00/$15.00   | $0.25/$1.25    | $8.00/$24.00  |",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "claude-3-opus-20240229",
          "data": [
            {
              "title": "Claude 3 Opus 200k",
              "value": "claude-3-opus-20240229",
              "context": 200000,
              "inputPrice": 0.15,
              "outputPrice": 0.75,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "Claude 3 Sonnet 200k",
              "value": "claude-3-sonnet-20240229",
              "context": 200000,
              "inputPrice": 0.03,
              "outputPrice": 0.15,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "Claude 3 Haiku 200k",
              "value": "claude-3-haiku-20240307",
              "inputPrice": 0.00025,
              "outputPrice": 0.00125,
              "context": 200000,
              "toolUse": true,
              "visionEnable": true
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "anthropicApiKey",
          "description": "Anthropic api key",
          "type": "password",
          "required": false,
          "title": "Claude Api Key",
          "default": "",
          "placeholder": "Claude Api Key"
        },
        {
          "name": "anthropicApiUrl",
          "description": "Anthropic api base url",
          "type": "textfield",
          "required": false,
          "title": "Anthropic Api Base Url",
          "default": "https://api.anthropic.com",
          "placeholder": "Anthropic Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "ollama_models",
      "title": "Ollama Models",
      "description": "get ollama model list",
      "icon": "ollama.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "llm",
          "description": "The model used to generate responses",
          "type": "extension",
          "required": false,
          "default": "default",
          "title": "LLM Model"
        }
      ]
    },
    {
      "name": "chat_lm_studio",
      "title": "LM Studio",
      "description": "Chat with LM Studio, learn more : [lmstudio.ai](https://lmstudio.ai/)",
      "icon": "lm_studio.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "baseUrl",
          "description": "LM Studio api base url",
          "type": "textfield",
          "required": false,
          "title": "LM Studio Api Base Url",
          "default": "http://127.0.0.1:1234/v1",
          "placeholder": "LM Studio Api Base Url"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_ollama",
      "title": "ChatOllama",
      "description": "Chat with Ollama, learn more : [ollama.ai](https://ollama.ai/)",
      "icon": "ollama.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "model",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama2:latest",
          "data": [],
          "dataProxy": "llm|ollama_models"
        },
        {
          "name": "baseUrl",
          "description": "Ollama api base url",
          "type": "textfield",
          "required": false,
          "title": "Ollama Api Base Url",
          "default": "http://127.0.0.1:11434",
          "placeholder": "Ollama Api Base Url"
        }
      ]
    },
    {
      "name": "chat_openrouter",
      "title": "OpenRouter",
      "description": "Chat with OpenRouter, learn more: [openrouter.ai](https://openrouter.ai/)",
      "icon": "openrouter.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "openai/gpt-3.5-turbo",
          "data": [
            {
              "title": "Openai Gpt-3.5-turbo",
              "value": "openai/gpt-3.5-turbo",
              "context": 16385,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "toolUse": true
            }
          ],
          "dataProxy": "llm|openrouter_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenRouter api key",
          "type": "password",
          "required": false,
          "title": "OpenRouter Api Key",
          "default": "",
          "placeholder": "OpenRouter Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenRouter api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenRouter Api Base Url",
          "default": "https://openrouter.ai/api/v1",
          "placeholder": "OpenRouter Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "openrouter_models",
      "title": "openrouter Models",
      "description": "get openrouter model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "chat_perplexity",
      "title": "Perplexity",
      "description": "Chat with Perplexity, learn more: [perplexity.ai](https://www.perplexity.ai/settings/api)",
      "icon": "perplexity.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "pplx-7b-online",
          "data": [],
          "dataProxy": "llm|perplexity_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "Perplexity api key",
          "type": "password",
          "required": false,
          "title": "Perplexity Api Key",
          "default": "",
          "placeholder": "Perplexity Api Key"
        },
        {
          "name": "frequencyPenalty",
          "description": "frequencyPenalty",
          "type": "textfield",
          "required": false,
          "title": "frequencyPenalty",
          "default": "1"
        },
        {
          "name": "baseUrl",
          "description": "Perplexity api base url",
          "type": "textfield",
          "required": false,
          "title": "Perplexity Api Base Url",
          "default": "https://api.perplexity.ai",
          "placeholder": "Perplexity Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "perplexity_models",
      "title": "perplexity Models",
      "description": "get perplexity model list",
      "icon": "perplexity.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "premai_models",
      "title": "premai Models",
      "description": "get premai model list",
      "icon": "premai.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "apiKey",
          "description": "api key",
          "type": "password",
          "required": false,
          "title": "Api Key",
          "default": "",
          "defaultProxy": "llm|premai",
          "placeholder": "Api Key"
        }
      ]
    },
    {
      "name": "chat_mistral",
      "title": "Mistral AI",
      "description": "Chat with Mistral AI, learn more : [mistral.ai](https://mistral.ai/)",
      "icon": "mistral.jpg",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "md:The model to generate the completion. \n\n ## Pricing and rate limits\n\n**Pay-as-you-go**\n\nThe prices listed below are exclusive of VAT.\n\n### Chat Completions API\n\n| Model           | Endpoint                | Input (USD)         | Output (USD)         |\n|-----------------|-------------------------|---------------------|----------------------|\n| Mistral 7B      | open-mistral-7b         | 0.25$ / 1M tokens   | 0.25$ / 1M tokens    |\n| Mixtral 8x7B    | open-mixtral-8x7b       | 0.7$ / 1M tokens    | 0.7$ / 1M tokens     |\n| Mistral Small   | mistral-small-latest     | 2$ / 1M tokens     | 6$ / 1M tokens       |\n| Mistral Medium  | mistral-medium-latest    | 2.7$ / 1M tokens   | 8.1$ / 1M tokens     |\n| Mistral Large   | mistral-large-latest     | 8$ / 1M tokens     | 24$ / 1M tokens      |\n",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "mistral-tiny",
          "data": [
            {
              "title": "Mistral Small",
              "value": "mistral-small-latest",
              "context": 16385,
              "inputPrice": 0.002,
              "outputPrice": 0.006
            },
            {
              "title": "Mistral Medium",
              "value": "mistral-medium-latest",
              "context": 16385,
              "inputPrice": 0.0027,
              "outputPrice": 0.0081
            },
            {
              "title": "Mistral Large",
              "value": "mistral-large-latest",
              "context": 16385,
              "inputPrice": 0.008,
              "outputPrice": 0.024
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "Mistral AI api key",
          "type": "password",
          "required": false,
          "title": "Mistral AI Api Key",
          "default": "",
          "placeholder": "Mistral AI Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_google",
      "title": "Google Gemini AI",
      "description": "Chat with Google Gemini AI, learn more : [Google AI](https://ai.google.dev/)",
      "icon": "google.jpg",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemini-pro",
          "data": [
            {
              "title": "Gemini 1.0 Pro",
              "value": "gemini-pro"
            },
            {
              "title": "Gemini 1.0 Pro Vision",
              "value": "Gemini 1.0 Pro Vision"
            },
            {
              "title": "Gemini 1.5 Pro",
              "value": "gemini-1.5-pro"
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "Google AI api key",
          "type": "password",
          "required": false,
          "title": "Google AI Api Key",
          "default": "",
          "placeholder": "Google AI Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_vertex",
      "title": "Google Vertex AI",
      "description": "Chat with Google Vertex AI, learn more : [Google Vertex](https://cloud.google.com/vertex-ai/docs),  [How to Authenticate](https://cloud.google.com/docs/authentication/client-libraries), Powered by [Langchain JS](https://js.langchain.com/docs/integrations/chat/google_vertex_ai)",
      "icon": "vertex.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemini-1.5-pro-preview-0409",
          "data": [
            {
              "title": "Gemini 1.0 Pro",
              "value": "gemini-1.0-pro",
              "context": 1000000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "visionEnable": true,
              "toolUse": true
            },
            {
              "title": "Gemini 1.0 Pro Vision",
              "value": "gemini-1.0-pro-vision",
              "context": 1000000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "visionEnable": true,
              "toolUse": true
            },
            {
              "title": "Gemini 1.5 Pro",
              "value": "gemini-1.5-pro-preview-0409",
              "context": 1000000,
              "inputPrice": 0.007,
              "outputPrice": 0.021,
              "visionEnable": true,
              "toolUse": true
            }
          ]
        },
        {
          "name": "project_id",
          "description": "Project ID",
          "type": "textfield",
          "required": false,
          "title": "Project ID"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_fireworks",
      "title": "Fireworks",
      "description": "Chat with Fireworks, learn more : [fireworks.ai](https://fireworks.ai), get all models from [fireworks models](https://fireworks.ai/models/fireworks/mixtral-8x7b-instruct)",
      "icon": "fireworks.jpg",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "textfield",
          "required": false,
          "title": "Model Name",
          "default": "accounts/fireworks/models/mixtral-8x7b-instruct"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "password",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.fireworks.ai/inference/v1",
          "placeholder": "OpenAI Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_cloudflare",
      "title": "Cloudflare Workers AI",
      "description": "Chat with Cloudflare Workers AI, learn more : [workers-ai](https://developers.cloudflare.com/workers-ai/)",
      "icon": "cloudflare.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "@cf/meta/llama-2-7b-chat-int8",
          "data": [],
          "dataProxy": "llm|cloudflare_models"
        },
        {
          "name": "cloudflareAccountId",
          "description": "Cloudflare AccountID",
          "type": "textfield",
          "required": false,
          "title": "Cloudflare AccountID",
          "default": "",
          "placeholder": "Cloudflare AccountID"
        },
        {
          "name": "cloudflareApiToken",
          "description": "Cloudflare api Token",
          "type": "password",
          "required": false,
          "title": "Cloudflare Api Token",
          "default": "",
          "placeholder": "Cloudflare Api Token"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "cloudflare_models",
      "title": "cloudflare Models",
      "description": "get cloudflare model list",
      "icon": "cloudflare.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "chat_together_ai",
      "title": "Together AI",
      "description": "Chat with together.ai, learn more : [together.ai](https://docs.together.ai/docs/quickstart), get chat models from [together models](https://docs.together.ai/docs/inference-models)",
      "icon": "together.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "textfield",
          "required": false,
          "title": "Model Name",
          "default": "mistralai/Mixtral-8x7B-Instruct-v0.1"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "TOGETHER_API_KEY",
          "type": "password",
          "required": false,
          "title": "TOGETHER_API_KEY",
          "default": "",
          "placeholder": "Together Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.together.xyz/v1",
          "placeholder": "OpenAI Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_moonshot_ai",
      "title": "MoonShot AI",
      "description": "Chat with moonshot ai, learn more : [moonshot.cn](https://www.moonshot.cn/)",
      "icon": "moonshot.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "moonshot-v1-8k",
          "data": [
            {
              "title": "moonshot-v1-8k",
              "value": "moonshot-v1-8k"
            },
            {
              "title": "moonshot-v1-32k",
              "value": "moonshot-v1-32k"
            },
            {
              "title": "moonshot-v1-128k",
              "value": "moonshot-v1-128k"
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "MOONSHOT_API_KEY",
          "type": "password",
          "required": false,
          "title": "MOONSHOT_API_KEY",
          "default": "",
          "placeholder": "MoonShot Api Key"
        },
        {
          "name": "baseUrl",
          "description": "MoonShot api base url",
          "type": "textfield",
          "required": false,
          "title": "MoonShot Api Base Url",
          "default": "https://api.moonshot.cn/v1",
          "placeholder": "MoonShot Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "chat_groq",
      "title": "Groq AI",
      "description": "Chat with Groq ai, learn more : [groq.com/](https://groq.com/)",
      "icon": "groq.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama2-70b-4096",
          "data": [
            {
              "title": "LLaMA2 70b",
              "value": "llama2-70b-4096",
              "context": 4096
            },
            {
              "title": "LLaMA3 8b",
              "value": "llama3-8b-8192",
              "context": 8192
            },
            {
              "title": "LLaMA3 70b",
              "value": "llama3-70b-8192",
              "context": 8192
            },
            {
              "title": "Mixtral 8x7b",
              "value": "mixtral-8x7b-32768",
              "context": 32768
            },
            {
              "title": "Gemma 7b",
              "value": "gemma-7b-it",
              "context": 8192
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "Groq_API_KEY",
          "type": "password",
          "required": false,
          "title": "Groq_API_KEY",
          "default": "",
          "placeholder": "Groq Api Key"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "premai",
      "title": "Prem AI",
      "description": "Chat using [PremAI](https://www.premai.io/) which provide LLM service",
      "icon": "premai.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "model",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gpt-3.5-turbo",
          "dataProxy": "llm|premai_models"
        },
        {
          "name": "apiKey",
          "description": "api key",
          "type": "password",
          "required": false,
          "title": "Api Key",
          "default": "",
          "placeholder": "Api Key"
        },
        {
          "name": "project_id",
          "description": "project_id",
          "type": "number",
          "required": false,
          "title": "Project ID",
          "default": "",
          "placeholder": "Project ID"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    },
    {
      "name": "cohere_models",
      "title": "cohere Models",
      "description": "get cohere model list",
      "icon": "cohere.jpg",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "apiKey",
          "description": "api key",
          "type": "password",
          "required": false,
          "title": "Api Key",
          "default": "",
          "defaultProxy": "llm|chat_cohere",
          "placeholder": "Api Key"
        }
      ]
    },
    {
      "name": "chat_cohere",
      "title": "Cohere AI",
      "description": "Chat using [Cohere](cohere.com), get [API KEY](https://dashboard.cohere.com/)",
      "icon": "cohere.jpg",
      "mode": "llm",
      "preferences": [
        {
          "name": "model",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "command-r-plus",
          "dataProxy": "llm|cohere_models",
          "data": [
            {
              "title": "Command-R-Plus (116k)",
              "value": "command-r-plus",
              "context": 116000
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "api key",
          "type": "password",
          "required": false,
          "title": "Api Key",
          "default": "",
          "placeholder": "Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming"
        }
      ]
    }
  ],
  "dependencies": {
    "@enconvo/api": "^0.1.125",
    "@google/generative-ai": "^0.5.0",
    "@langchain/anthropic": "^0.1.16",
    "@langchain/cloudflare": "^0.0.4",
    "@langchain/cohere": "^0.0.6",
    "@langchain/community": "^0.0.44",
    "@langchain/core": "^0.1.60",
    "@langchain/google-genai": "^0.0.11",
    "@langchain/google-vertexai": "^0.0.5",
    "@langchain/groq": "^0.0.1",
    "@langchain/mistralai": "^0.0.5",
    "@langchain/openai": "^0.0.26",
    "@premai/prem-sdk": "^0.3.41",
    "langchain": "^0.1.36"
  },
  "devDependencies": {
    "@types/node": "^18.19.31",
    "eslint": "^8.57.0",
    "prettier": "^2.8.8",
    "tsup": "^7.2.0",
    "typescript": "^5.4.5"
  },
  "scripts": {
    "lint": "eslint src",
    "lint:fix": "npm run lint --fix",
    "format": "prettier --write \"**/*.ts\"",
    "format:check": "prettier --list-different \"**/*.ts\"",
    "build": "enconvo",
    "dev": "enconvo --dev"
  },
  "minAppVersion": "1.8.8"
}