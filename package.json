{
  "$schema": "https://enconvo.com/schemas/extension.json",
  "name": "llm",
  "version": "1.3.112",
  "description": "Large Language Models (LLM) services providers",
  "title": "LLM Providers",
  "icon": "icon.png",
  "author": "ysnows",
  "license": "MIT",
  "categories": [
    "Large Language Model"
  ],
  "group": "llm",
  "type": "module",
  "commands": [
    {
      "name": "enconvo_ai",
      "title": "Enconvo Cloud Plan",
      "description": "Chat using EnconvoAI which provide LLM service, , learn more : [docs](https://docs.enconvo.com/docs/providers/llm)",
      "icon": "enconvo.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "md:The model to generate the completion. Every call will consume points, and the specific consumption details are as followsï¼š\n\n  **OpenAI:** \n\n | Model Name | Points |\n | --- | --- |\n | gpt-35-turbo | 50 |\n | gpt4 | 875 |\n\n\n | gpt4 | 875 |\n\n\n | gpt4-vision | 1250 |\n\n\n | Claude 3 Sonnet | 1250 |\n\n\n | Claude 3 Opus | 6000 |\n\n",
          "type": "dropdown",
          "required": false,
          "title": "Model Name Anthropic",
          "default": "openai/gpt-4o-mini",
          "data": [
            {
              "title": "OpenAI: gpt-4o-mini 128k (10 points per message)",
              "value": "openai/gpt-4o-mini",
              "context": 128000,
              "toolUse": true,
              "visionEnable": true
            }
          ],
          "dataProxy": "llm|enconvo_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "hidden",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "default",
          "placeholder": "sk-********"
        },
        {
          "name": "anthropicApiKey",
          "description": "OpenAI api key",
          "type": "hidden",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "default",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "apiKey",
          "description": "google api key",
          "type": "hidden",
          "required": false,
          "title": "google Api Key",
          "default": "default",
          "placeholder": "google Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api key",
          "type": "hidden",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "https://api.enconvo.com/v1/",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "anthropicApiUrl",
          "description": "OpenAI api key",
          "type": "hidden",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "https://api.enconvo.com/",
          "placeholder": "OpenAI Api Key"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_open_ai",
      "title": "OpenAI",
      "description": "Chat with OpenAI , learn more : [openai developers](https://platform.openai.com/docs/overview)",
      "icon": "openai.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gpt-4o-mini",
          "data": [
            {
              "title": "gpt-4o-mini",
              "value": "gpt-4o-mini",
              "context": 128000,
              "inputPrice": 0.00015,
              "outputPrice": 0.000075,
              "toolUse": true,
              "visionEnable": true
            }
          ],
          "dataProxy": "llm|openai_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenAI api key",
          "type": "password",
          "required": false,
          "title": "OpenAI Api Key",
          "default": "",
          "defaultProxy": "KEY_OPENAI_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "defaultProxy": "KEY_OPENAI_BASEURL",
          "default": "https://api.openai.com/v1",
          "placeholder": "OpenAI Api Base Url"
        },
        {
          "name": "customHeaders",
          "description": "Custom Headers, In Json Format",
          "type": "textfield",
          "required": false,
          "title": "Custom Headers",
          "default": "{}",
          "placeholder": "{}"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_anthropic",
      "title": "Anthropic Claude",
      "description": "Chat with Anthropic Claude, learn more : [anthropic.com](https://www.anthropic.com/)",
      "mode": "llm",
      "icon": "anthropic.png",
      "preferences": [
        {
          "name": "modelName",
          "description": "md: The model to generate the completion. \n|               | Claude 3 Opus | Claude 3 Sonnet | Claude 3 Haiku |\n|---------------|--------------|----------------|---------------|\n| Context window| 200K*        | 200K*          | 200K*         |\n| Cost (Input / Output per MTok^)| $15.00/$75.00 | $3.00/$15.00   | $0.25/$1.25    | $8.00/$24.00  |",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "claude-3-opus-20240229",
          "data": [
            {
              "title": "Claude 3.5 Sonnet New 200k",
              "value": "claude-3-5-sonnet-latest",
              "context": 200000,
              "inputPrice": 0.03,
              "outputPrice": 0.15,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "Claude 3.5 Haiku New 200k",
              "value": "claude-3-5-haiku-20241022",
              "context": 200000,
              "inputPrice": 0.003,
              "outputPrice": 0.15,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "Claude 3.5 Sonnet 20240620 200k",
              "value": "claude-3-5-sonnet-20240620",
              "context": 200000,
              "inputPrice": 0.03,
              "outputPrice": 0.15,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "Claude 3 Opus 200k",
              "value": "claude-3-opus-20240229",
              "context": 200000,
              "inputPrice": 0.15,
              "outputPrice": 0.75,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "Claude 3 Sonnet 200k",
              "value": "claude-3-sonnet-20240229",
              "context": 200000,
              "inputPrice": 0.03,
              "outputPrice": 0.15,
              "toolUse": true,
              "visionEnable": true
            },
            {
              "title": "Claude 3 Haiku 200k",
              "value": "claude-3-haiku-20240307",
              "inputPrice": 0.00025,
              "outputPrice": 0.00125,
              "context": 200000,
              "toolUse": true,
              "visionEnable": true
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "anthropicApiKey",
          "description": "Anthropic api key",
          "type": "password",
          "required": false,
          "title": "Claude Api Key",
          "default": "",
          "defaultProxy": "KEY_CLAUDE_APIKEY",
          "placeholder": "sk-ant-********"
        },
        {
          "name": "anthropicApiUrl",
          "description": "Anthropic api base url",
          "type": "textfield",
          "required": false,
          "title": "Anthropic Api Base Url",
          "defaultProxy": "KEY_CLAUDE_BASEURL",
          "default": "https://api.anthropic.com",
          "placeholder": "Anthropic Api Base Url"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_google",
      "title": "Google Gemini AI",
      "description": "Chat with Google Gemini AI, learn more : [Google AI](https://ai.google.dev/)",
      "icon": "google.jpg",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemini-1.5-flash-002",
          "data": [
            {
              "title": "Gemini 1.5 Flash-8B",
              "value": "gemini-1.5-flash-8b",
              "context": 1048576,
              "visionEnable": true
            },
            {
              "title": "Gemini 1.5 Flash 002",
              "value": "gemini-1.5-flash-002",
              "context": 1048576,
              "visionEnable": true
            },
            {
              "title": "Gemini 1.5 Pro 002",
              "value": "gemini-1.5-pro-002",
              "context": 2097152,
              "visionEnable": true
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "Google AI api key",
          "type": "password",
          "required": false,
          "title": "Google AI Api Key",
          "default": "",
          "defaultProxy": "KEY_GEMINI_APIKEY",
          "placeholder": "AI********"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "azure_openai",
      "title": "Azure OpenAI",
      "description": "Azure OpenAI , learn more : [azure openai](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)",
      "icon": "azure.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "Azure OpenAI api deployment name",
          "type": "dropdown",
          "required": false,
          "title": "Azure OpenAI Api Deployment Name",
          "default": "gpt-4o",
          "dataProxy": "llm|openai_models",
          "data": [
            {
              "title": "gpt-4o",
              "value": "gpt-4o",
              "context": 128000,
              "inputPrice": 0.005,
              "outputPrice": 0.015,
              "toolUse": true,
              "visionEnable": true
            }
          ]
        },
        {
          "name": "azureOpenAIApiInstanceName",
          "description": "Azure OpenAI api instance name",
          "type": "textfield",
          "required": false,
          "title": "Instance Name",
          "default": "",
          "defaultProxy": "KEY_AZURE_OPENAI_INSTANCE_NAME"
        },
        {
          "name": "azureOpenAIApiVersion",
          "type": "textfield",
          "required": false,
          "title": "API Version",
          "defaultProxy": "KEY_AZURE_OPENAI_API_VERSION",
          "default": "2024-02-15-preview"
        },
        {
          "name": "azureOpenAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://azure.microsoft.com/en-us/products/ai-services/openai-service)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_AZURE_OPENAI_APIKEY",
          "placeholder": "Azure OpenAI Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_groq",
      "title": "Groq AI",
      "description": "Chat with Groq ai, learn more : [groq.com/](https://groq.com/)",
      "icon": "groq.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemma2-9b-it",
          "dataProxy": "llm|groq_models",
          "data": []
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "How to get api key? [ðŸ”‘here](https://console.groq.com/keys)",
          "type": "password",
          "required": false,
          "title": "API KEY",
          "default": "",
          "defaultProxy": "KEY_GROQ_APIKEY",
          "placeholder": "gsk_********",
          "visibility": "hidden"
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://console.groq.com/keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_GROQ_APIKEY",
          "placeholder": "gsk_********"
        },
        {
          "name": "baseUrl",
          "description": "Groq api base url",
          "type": "textfield",
          "required": false,
          "title": "Groq Api Base Url",
          "default": "https://api.groq.com/openai/v1",
          "placeholder": "Groq Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_x_ai",
      "title": "X.AI-GROK",
      "description": "Chat with Grok AI, learn more : [x.ai](https://x.ai/)",
      "icon": "x.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "grok-beta",
          "data": [
            {
              "title": "Grok-Beta",
              "value": "grok-beta",
              "context": 128000,
              "inputPrice": 0.0005,
              "outputPrice": 0.015
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://docs.x.ai/docs/quickstart#creating-an-api-key)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_XAI_APIKEY",
          "placeholder": "xai-********"
        },
        {
          "name": "baseUrl",
          "description": "X.AI api base url",
          "type": "textfield",
          "required": false,
          "title": "X.AI Api Base Url",
          "default": "https://api.x.ai/v1",
          "placeholder": "X.AI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_mistral",
      "title": "Mistral AI",
      "description": "Chat with Mistral AI, learn more : [mistral.ai](https://mistral.ai/)",
      "icon": "mistral.jpg",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "md:The model to generate the completion. \n\n ## Pricing and rate limits\n\n**Pay-as-you-go**\n\nThe prices listed below are exclusive of VAT.\n\n### Chat Completions API\n\n| Model           | Endpoint                | Input (USD)         | Output (USD)         |\n|-----------------|-------------------------|---------------------|----------------------|\n| Mistral 7B      | open-mistral-7b         | 0.25$ / 1M tokens   | 0.25$ / 1M tokens    |\n| Mixtral 8x7B    | open-mixtral-8x7b       | 0.7$ / 1M tokens    | 0.7$ / 1M tokens     |\n| Mistral Small   | mistral-small-latest     | 2$ / 1M tokens     | 6$ / 1M tokens       |\n| Mistral Medium  | mistral-medium-latest    | 2.7$ / 1M tokens   | 8.1$ / 1M tokens     |\n| Mistral Large   | mistral-large-latest     | 8$ / 1M tokens     | 24$ / 1M tokens      |\n",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "mistral-small-latest",
          "data": [
            {
              "title": "Mistral Small",
              "value": "mistral-small-latest",
              "context": 32000,
              "inputPrice": 0.0002,
              "outputPrice": 0.0006
            },
            {
              "title": "Mistral Large",
              "value": "mistral-large-latest",
              "context": 128000,
              "inputPrice": 0.002,
              "outputPrice": 0.006
            },
            {
              "title": "Codestral",
              "value": "codestral-latest",
              "context": 32000,
              "inputPrice": 0.0002,
              "outputPrice": 0.0006
            },
            {
              "title": "Pixtral",
              "value": "pixtral-12b-2409",
              "context": 128000,
              "visionEnable": true,
              "inputPrice": 0.00015,
              "outputPrice": 0.00015
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "How to get api key? [ðŸ”‘here](https://console.mistral.ai/api-keys/)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_MISTRAL_APIKEY",
          "placeholder": "Mistral AI Api Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_ollama",
      "title": "Ollama",
      "description": "Chat with Ollama, learn more : [ollama.ai](https://ollama.ai/)",
      "icon": "ollama.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "model",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama2:latest",
          "data": [],
          "dataProxy": "llm|ollama_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "baseUrl",
          "description": "Ollama api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "http://127.0.0.1:11434",
          "placeholder": "http://127.0.0.1:11434",
          "defaultProxy": "KEY_OLLAMA_BASEURL"
        }
      ]
    },
    {
      "name": "chat_cohere",
      "title": "Cohere AI",
      "description": "Chat using [Cohere](cohere.com), get [API KEY](https://dashboard.cohere.com/)",
      "icon": "cohere.jpg",
      "mode": "llm",
      "preferences": [
        {
          "name": "model",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "command-r-plus",
          "dataProxy": "llm|cohere_models",
          "data": [
            {
              "title": "Command-R-Plus (116k)",
              "value": "command-r-plus",
              "context": 116000
            }
          ]
        },
        {
          "name": "apiKey",
          "description": "How to get api key? [ðŸ”‘here](https://dashboard.cohere.com/api-keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_COHERE_APIKEY",
          "placeholder": "API Key"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_perplexity",
      "title": "Perplexity",
      "description": "Chat with Perplexity, learn more: [perplexity.ai](https://www.perplexity.ai/settings/api)",
      "icon": "perplexity.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama-3.1-sonar-small-128k-online",
          "data": [],
          "dataProxy": "llm|perplexity_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://www.perplexity.ai/settings/api)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_PERPLEXITY_APIKEY",
          "placeholder": "pplx-********"
        },
        {
          "name": "frequencyPenalty",
          "description": "frequencyPenalty",
          "type": "textfield",
          "required": false,
          "title": "frequencyPenalty",
          "default": "1",
          "visibility": "hidden"
        },
        {
          "name": "baseUrl",
          "description": "Perplexity api base url",
          "type": "textfield",
          "required": false,
          "title": "Perplexity Api Base Url",
          "default": "https://api.perplexity.ai",
          "placeholder": "Perplexity Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "deepseek",
      "title": "Deep Seek",
      "description": "Chat with DeepSeek , learn more : [DeepSeek developers](https://platform.deepseek.com/)",
      "icon": "deepseek.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "deepseek-chat",
          "data": [
            {
              "title": "Deepseek Chat",
              "value": "deepseek-chat",
              "context": 32000,
              "inputPrice": 0.00014,
              "outputPrice": 0.00028
            },
            {
              "title": "Deepseek Coder",
              "value": "deepseek-coder",
              "context": 16000,
              "inputPrice": 0.00014,
              "outputPrice": 0.00028
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://platform.deepseek.com/api_keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_DEEPSEEK_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.deepseek.com",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "siliconflow",
      "title": "SiliconFlow",
      "description": "Chat with SiliconFlow , learn more : [SiliconFlow developers](https://siliconflow.cn/)",
      "icon": "siliconflow.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "Qwen/Qwen2-7B-Instruct",
          "data": [
            {
              "title": "Qwen/Qwen2-7B-Instruct",
              "value": "Qwen/Qwen2-7B-Instruct",
              "context": 32000,
              "inputPrice": 0.00014,
              "outputPrice": 0.00028
            }
          ],
          "dataProxy": "llm|silliconflow_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://cloud.siliconflow.cn/account/ak)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_SILICONFLOW_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.siliconflow.cn/v1",
          "placeholder": "Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_lm_studio",
      "title": "LM Studio",
      "description": "Chat with LM Studio, learn more : [lmstudio.ai](https://lmstudio.ai/)",
      "icon": "lm_studio.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "baseUrl",
          "description": "LM Studio api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "http://127.0.0.1:1234/v1",
          "defaultProxy": "KEY_LMSTUDIO_BASEURL",
          "placeholder": "LM Studio Api Base Url"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_openrouter",
      "title": "OpenRouter",
      "description": "Chat with OpenRouter, learn more: [openrouter.ai](https://openrouter.ai/)",
      "icon": "openrouter.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "anthropic/claude-3.5-sonnet",
          "data": [
            {
              "title": "Openai Gpt-3.5-turbo",
              "value": "openai/gpt-3.5-turbo",
              "context": 16385,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "toolUse": true
            }
          ],
          "dataProxy": "llm|openrouter_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "OpenRouter api key",
          "type": "password",
          "required": false,
          "title": "OpenRouter Api Key",
          "default": "",
          "defaultProxy": "KEY_OPENROUTER_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenRouter api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenRouter Api Base Url",
          "default": "https://openrouter.ai/api/v1",
          "placeholder": "OpenRouter Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_vertex",
      "title": "Google Vertex AI",
      "description": "Chat with Google Vertex AI, learn more : [Google Vertex](https://cloud.google.com/vertex-ai/docs),  [How to Authenticate](https://cloud.google.com/docs/authentication/client-libraries), Powered by [Langchain JS](https://js.langchain.com/docs/integrations/chat/google_vertex_ai)",
      "icon": "vertex.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemini-1.5-flash-002",
          "data": [
            {
              "title": "Gemini 1.5 Flash 002",
              "value": "gemini-1.5-flash-002",
              "context": 1048576,
              "visionEnable": true
            },
            {
              "title": "Gemini 1.5 Pro 002",
              "value": "gemini-1.5-pro-002",
              "context": 2097152,
              "visionEnable": true
            },
            {
              "title": "Claude 3.5 Sonnet",
              "value": "claude-3-5-sonnet@20240620",
              "context": 200000,
              "visionEnable": true
            }
          ]
        },
        {
          "name": "project_id",
          "description": "How to get project id? [ðŸ”‘here](https://cloud.google.com/resource-manager/docs/creating-managing-projects)",
          "type": "textfield",
          "required": false,
          "title": "Project ID",
          "default": "",
          "defaultProxy": "KEY_GOOGLE_PROJECT_ID",
          "placeholder": "Google Project ID"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_fireworks",
      "title": "Fireworks",
      "description": "Chat with Fireworks, learn more : [fireworks.ai](https://fireworks.ai), get all models from [fireworks models](https://fireworks.ai/models/fireworks/mixtral-8x7b-instruct)",
      "icon": "fireworks.jpg",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "textfield",
          "required": false,
          "title": "Model Name",
          "default": "accounts/fireworks/models/llama-v3p2-3b-instruct"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://fireworks.ai/account/api-keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_FIREWORKS_APIKEY",
          "placeholder": "API Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.fireworks.ai/inference/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_cloudflare",
      "title": "Cloudflare Workers AI",
      "description": "Chat with Cloudflare Workers AI, learn more : [workers-ai](https://developers.cloudflare.com/workers-ai/)",
      "icon": "cloudflare.png",
      "mode": "llm",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "@cf/meta/llama-2-7b-chat-int8",
          "data": [],
          "dataProxy": "llm|cloudflare_models"
        },
        {
          "name": "cloudflareAccountId",
          "description": "Cloudflare AccountID",
          "type": "textfield",
          "required": false,
          "title": "Cloudflare AccountID",
          "default": "",
          "defaultProxy": "KEY_CLOUDFLARE_ACCOUNTID",
          "placeholder": "Cloudflare AccountID"
        },
        {
          "name": "cloudflareApiToken",
          "description": "Cloudflare API Token",
          "type": "password",
          "required": false,
          "title": "Cloudflare API Token",
          "default": "",
          "defaultProxy": "KEY_CLOUDFLARE_APIKEY",
          "placeholder": "Cloudflare API Token"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_together_ai",
      "title": "Together AI",
      "description": "Chat with together.ai, learn more : [together.ai](https://docs.together.ai/docs/quickstart), get chat models from [together models](https://docs.together.ai/docs/inference-models)",
      "icon": "together.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "textfield",
          "required": false,
          "title": "Model Name",
          "default": "meta-llama/Llama-3.2-3B-Instruct-Turbo"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://docs.together.ai/docs/quickstart)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_TOGETHER_APIKEY",
          "placeholder": "Together Api Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.together.xyz/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_moonshot_ai",
      "title": "MoonShot AI",
      "description": "Chat with moonshot ai, learn more : [moonshot.cn](https://www.moonshot.cn/)",
      "icon": "moonshot.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "moonshot-v1-8k",
          "data": [
            {
              "title": "moonshot-v1-8k",
              "value": "moonshot-v1-8k"
            },
            {
              "title": "moonshot-v1-32k",
              "value": "moonshot-v1-32k"
            },
            {
              "title": "moonshot-v1-128k",
              "value": "moonshot-v1-128k"
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "MOONSHOT_API_KEY",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_MOONSHOT_APIKEY",
          "placeholder": "MoonShot Api Key"
        },
        {
          "name": "baseUrl",
          "description": "MoonShot api base url",
          "type": "textfield",
          "required": false,
          "title": "MoonShot Api Base Url",
          "default": "https://api.moonshot.cn/v1",
          "placeholder": "MoonShot Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "ollama_models",
      "title": "Ollama Models",
      "description": "get ollama model list",
      "icon": "ollama.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "baseUrl",
          "description": "Ollama api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "http://127.0.0.1:11434",
          "placeholder": "http://127.0.0.1:11434",
          "defaultProxy": "KEY_OLLAMA_BASEURL"
        }
      ]
    },
    {
      "name": "openrouter_models",
      "title": "openrouter Models",
      "description": "get openrouter model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "perplexity_models",
      "title": "perplexity Models",
      "description": "get perplexity model list",
      "icon": "perplexity.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "premai_models",
      "title": "premai Models",
      "description": "get premai model list",
      "icon": "premai.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "apiKey",
          "description": "api key",
          "type": "password",
          "required": false,
          "title": "Api Key",
          "default": "",
          "defaultProxy": "llm|premai",
          "placeholder": "Api Key"
        }
      ]
    },
    {
      "name": "groq_models",
      "title": "groq Models",
      "description": "get groq model list",
      "icon": "groq.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "enconvo_models",
      "title": "enconvo Models",
      "description": "get enconvo model list",
      "icon": "enconvo.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "openai_models",
      "title": "openai Models",
      "description": "get openai model list",
      "icon": "openai.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "cloudflare_models",
      "title": "cloudflare Models",
      "description": "get cloudflare model list",
      "icon": "cloudflare.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "cohere_models",
      "title": "cohere Models",
      "description": "get cohere model list",
      "icon": "cohere.jpg",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "apiKey",
          "description": "How to get api key? [ðŸ”‘here](https://dashboard.cohere.com/api-keys)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_COHERE_APIKEY",
          "placeholder": "API Key"
        }
      ]
    },
    {
      "name": "chat_yii",
      "title": "Yi",
      "description": "Chat with Yi , learn more : [Yi developers](https://platform.lingyiwanwu.com/)",
      "icon": "yi.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "yi-34b-chat-0205",
          "data": [
            {
              "title": "yi-large",
              "value": "yi-large",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-medium",
              "value": "yi-medium",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-vision",
              "value": "yi-vision",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-large-turbo",
              "value": "yi-large-turbo",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-34b-chat-0205",
              "value": "yi-34b-chat-0205",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-34b-chat-200k",
              "value": "yi-34b-chat-200k",
              "context": 200000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "maxTokens": 4000
            },
            {
              "title": "yi-vl-plus",
              "value": "yi-vl-plus",
              "context": 4000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "visionEnable": true,
              "maxTokens": 2000
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "api key",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_YI_APIKEY",
          "placeholder": "API Key"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://api.lingyiwanwu.com/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_dashscope",
      "title": "DashScope",
      "description": "Chat with é˜¿é‡Œäº‘ç™¾ç‚¼ , learn more : [é˜¿é‡Œäº‘ç™¾ç‚¼](https://bailian.console.aliyun.com/#/home)",
      "icon": "dashscope.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "qwen-max-latest",
          "data": [
            {
              "title": "Qwen-Max-Latest",
              "value": "qwen-max-latest",
              "context": 32000
            },
            {
              "title": "Qwen-Plus-Latest",
              "value": "qwen-plus-latest",
              "context": 128000
            },
            {
              "title": "Qwen-Turbo-Latest",
              "value": "qwen-turbo-latest",
              "context": 128000
            },
            {
              "title": "Qwen-VL-Max-Latest",
              "value": "qwen-vl-max-latest",
              "context": 8000,
              "visionEnable": true
            },
            {
              "title": "Qwen-VL-Max-Plus",
              "value": "qwen-vl-max-plus",
              "context": 8000,
              "visionEnable": true
            },
            {
              "title": "Qwen2.5-72B",
              "value": "qwen2.5-72b-instruct",
              "context": 128000
            },
            {
              "title": "Qwen2.5-32B",
              "value": "qwen2.5-32b-instruct",
              "context": 128000
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://bailian.console.aliyun.com/#/home)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_DASHSCOPE_APIKEY",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "OpenAI api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://dashscope.aliyuncs.com/compatible-mode/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_sambanova",
      "title": "Sambanova AI",
      "description": "Chat with sambanova.ai, learn more : [sambanova.ai](https://cloud.sambanova.ai/pricing)",
      "icon": "sambanova.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "Meta-Llama-3.2-3B-Instruct",
          "data": [
            {
              "title": "Meta-Llama-3.1-8B-Instruct",
              "value": "Meta-Llama-3.1-8B-Instruct",
              "context": 16000
            },
            {
              "title": "Meta-Llama-3.1-70B-Instruct",
              "value": "Meta-Llama-3.1-70B-Instruct",
              "context": 64000
            },
            {
              "title": "Meta-Llama-3.1-405B-Instruct",
              "value": "Meta-Llama-3.1-405B-Instruct",
              "context": 4096
            },
            {
              "title": "Meta-Llama-3.2-1B-Instruct",
              "value": "Meta-Llama-3.2-1B-Instruct",
              "context": 4096
            },
            {
              "title": "Meta-Llama-3.2-3B-Instruct",
              "value": "Meta-Llama-3.2-3B-Instruct",
              "context": 4096
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://cloud.sambanova.ai/apis)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_SAMBANOVA_APIKEY",
          "placeholder": "********"
        },
        {
          "name": "baseUrl",
          "description": "Perplexity api base url",
          "type": "textfield",
          "required": false,
          "title": "Perplexity Api Base Url",
          "default": "https://api.sambanova.ai/v1",
          "placeholder": "Perplexity Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_cerebras",
      "title": "Cerebras AI",
      "description": "Chat with cerebras.ai, learn more : [cerebras.ai](https://cerebras.ai/)",
      "icon": "cerebras.png",
      "mode": "llm",
      "targetCommand": "llm/chat_open_ai",
      "preferences": [
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama3.1-8b",
          "data": [
            {
              "title": "Llama 3.1 8B",
              "value": "llama3.1-8b",
              "context": 8192
            },
            {
              "title": "Llama 3.1 70B",
              "value": "llama3.1-70b",
              "context": 8192
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": "1.0",
          "data": [
            {
              "title": "none",
              "value": "0.0"
            },
            {
              "title": "low",
              "value": "0.5"
            },
            {
              "title": "medium",
              "value": "1.0"
            },
            {
              "title": "high",
              "value": "1.5"
            },
            {
              "title": "maximum",
              "value": "2.0"
            }
          ]
        },
        {
          "name": "openAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://inference-docs.cerebras.ai/openai)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "KEY_CEREBRAS_APIKEY",
          "placeholder": "********"
        },
        {
          "name": "baseUrl",
          "description": "Perplexity api base url",
          "type": "textfield",
          "required": false,
          "title": "Perplexity Api Base Url",
          "default": "https://api.cerebras.ai/v1",
          "placeholder": "Perplexity Api Base Url",
          "visibility": "hidden"
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "silliconflow_models",
      "title": "silliconflow_models",
      "description": "get silliconflow model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "openAIApiKey",
          "description": "api key",
          "type": "password",
          "required": false,
          "title": "Api Key",
          "default": "",
          "placeholder": "Api Key",
          "defaultProxy": "llm|siliconflow"
        }
      ]
    }
  ],
  "dependencies": {
    "@enconvo/api": "^0.1.131",
    "@google/generative-ai": "^0.5.0",
    "@langchain/anthropic": "^0.1.21",
    "@langchain/cloudflare": "^0.0.4",
    "@langchain/cohere": "^0.0.6",
    "@langchain/community": "^0.3.4",
    "@langchain/core": "^0.3.8",
    "@langchain/google-genai": "^0.1.0",
    "@langchain/google-vertexai": "^0.1.0",
    "@langchain/groq": "^0.0.1",
    "@langchain/mistralai": "^0.0.5",
    "@langchain/openai": "^0.3.2",
    "@premai/prem-sdk": "^0.3.69",
    "langchain": "^0.1.37"
  },
  "devDependencies": {
    "@types/node": "^18.19.51",
    "eslint": "^8.57.1",
    "prettier": "^2.8.8",
    "tsup": "^7.2.0",
    "typescript": "^5.6.2"
  },
  "scripts": {
    "lint": "eslint src",
    "lint:fix": "npm run lint --fix",
    "format": "prettier --write \"**/*.ts\"",
    "format:check": "prettier --list-different \"**/*.ts\"",
    "build": "enconvo",
    "dev": "enconvo --dev"
  },
  "minAppVersion": "1.8.8"
}
