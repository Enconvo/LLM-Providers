{
  "$schema": "https://enconvo.com/schemas/extension.json",
  "name": "llm",
  "version": "1.3.217",
  "description": "Large Language Models (LLM) services providers",
  "title": "AI Model Providers",
  "icon": "icon.png",
  "author": "ysnows",
  "license": "MIT",
  "categories": [
    "Provider"
  ],
  "group": "llm",
  "type": "module",
  "commands": [
    {
      "name": "enconvo_ai",
      "title": "Enconvo Cloud Plan",
      "description": "Chat using EnconvoAI which provide LLM service, , learn more : [docs](https://docs.enconvo.com/docs/providers/llm)",
      "icon": "enconvo.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "enconvo_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|enconvo_ai"
            ]
          },
          "title": "Credential Provider",
          "visibility": "hidden"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion. Every call will consume points",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "anthropic/claude-sonnet-4-20250514",
          "dataProxy": "llm|enconvo_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "reasoning_effort_o",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "low",
          "data": [
            {
              "title": "low",
              "value": "low"
            },
            {
              "title": "medium",
              "value": "medium"
            },
            {
              "title": "high",
              "value": "high"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "openai/o1-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o1",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o1-pro",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o3-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o3",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/o4-mini",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "minimal",
          "data": [
            {
              "title": "minimal",
              "value": "minimal"
            },
            {
              "title": "low",
              "value": "low"
            },
            {
              "title": "medium",
              "value": "medium"
            },
            {
              "title": "high",
              "value": "high"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "openai/gpt-5-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/gpt-5",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "openai/gpt-5-nano",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "chat_open_ai",
      "title": "OpenAI",
      "description": "Chat with OpenAI , learn more : [openai developers](https://platform.openai.com/docs/overview)",
      "icon": "openai.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "open_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|open_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gpt-4.1",
          "dataProxy": "llm|openai_models",
          "dataCustomTitle": "Add Custom Model"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "reasoning_effort_o",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "low",
          "data": [
            {
              "title": "low",
              "value": "low"
            },
            {
              "title": "medium",
              "value": "medium"
            },
            {
              "title": "high",
              "value": "high"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "o1-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o1",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o1-pro",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o3-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o3",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "o4-mini",
                  "operator": "or"
                }
              ]
            }
          }
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "minimal",
          "data": [
            {
              "title": "minimal",
              "value": "minimal"
            },
            {
              "title": "low",
              "value": "low"
            },
            {
              "title": "medium",
              "value": "medium"
            },
            {
              "title": "high",
              "value": "high"
            }
          ],
          "showOnlyWhen": {
            "modelName": {
              "operator": "or",
              "conditions": [
                {
                  "condition": "eq",
                  "value": "gpt-5-mini",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gpt-5",
                  "operator": "or"
                },
                {
                  "condition": "eq",
                  "value": "gpt-5-nano",
                  "operator": "or"
                }
              ]
            }
          }
        }
      ]
    },
    {
      "name": "anthropic_models",
      "title": "Anthropic Models",
      "description": "get anthropic model list",
      "commandType": "function_command",
      "icon": "anthropic.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "anthropic",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|anthropic"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "chat_anthropic",
      "title": "Anthropic Claude",
      "description": "Chat with Anthropic Claude, learn more : [anthropic.com](https://www.anthropic.com/)",
      "mode": "no-view",
      "commandType": "provider",
      "icon": "anthropic.png",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "anthropic",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|anthropic"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "md: The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "claude-sonnet-4-20250514",
          "dataProxy": "llm|anthropic_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            }
          ]
        }
      ]
    },
    {
      "name": "chat_google",
      "title": "Google Gemini AI",
      "description": "Chat with Google Gemini AI, learn more : [Google AI](https://ai.google.dev/)",
      "icon": "google.jpg",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "gemini",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|gemini"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemini-2.5-flash-preview-04-17",
          "dataProxy": "llm|gemini_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "deepseek",
      "title": "Deep Seek",
      "description": "Chat with DeepSeek , learn more : [DeepSeek developers](https://platform.deepseek.com/)",
      "icon": "deepseek.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "deepseek",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|deepseek"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "deepseek-chat",
          "dataProxy": "llm|deepseek_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 0.6,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "reasoning_effort",
          "description": "Applicable to reasoning models only, this option controls the reasoning token length.",
          "type": "dropdown",
          "required": false,
          "title": "Reasoning Effort",
          "default": "off",
          "data": [
            {
              "title": "low",
              "value": "low"
            },
            {
              "title": "medium",
              "value": "medium"
            },
            {
              "title": "high",
              "value": "high"
            },
            {
              "title": "off",
              "value": "off"
            }
          ]
        }
      ]
    },
    {
      "name": "chat_groq",
      "title": "Groq AI",
      "description": "Chat with Groq ai, learn more : [groq.com/](https://groq.com/)",
      "icon": "groq.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "groq",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|groq"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gemma2-9b-it",
          "dataProxy": "llm|groq_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "azure_openai",
      "title": "Azure OpenAI",
      "description": "Azure OpenAI , learn more : [azure openai](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)",
      "icon": "azure.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "azureOpenAIApiKey",
          "description": "How to get api key? [ðŸ”‘here](https://azure.microsoft.com/en-us/products/ai-services/openai-service)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "credentials|azure_openai",
          "placeholder": "Azure OpenAI Api Key"
        },
        {
          "name": "azureOpenAIApiEndpoint",
          "description": "Azure OpenAI api endpoint",
          "type": "textfield",
          "required": false,
          "title": "Endpoint",
          "default": "",
          "defaultProxy": "credentials|azure_openai"
        },
        {
          "name": "azureOpenAIApiVersion",
          "description": "Azure OpenAI api version",
          "type": "textfield",
          "required": false,
          "title": "API Version",
          "defaultProxy": "credentials|azure_openai",
          "default": "2024-02-15-preview"
        },
        {
          "name": "modelName",
          "description": "Azure OpenAI api deployment name",
          "type": "dropdown",
          "required": false,
          "title": "Azure OpenAI Api Deployment Name",
          "default": "gpt-4o",
          "dataProxy": "llm|azureai_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "streaming",
          "description": "Whether to stream output",
          "type": "checkbox",
          "required": false,
          "title": "Streaming",
          "default": true,
          "label": "Streaming",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "chat_x_ai",
      "title": "X.AI-GROK",
      "description": "Chat with Grok AI, learn more : [x.ai](https://x.ai/)",
      "icon": "x.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "x_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|x_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "grok-4-0709",
          "dataProxy": "llm|x_ai_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_mistral",
      "title": "Mistral AI",
      "description": "Chat with Mistral AI, learn more : [mistral.ai](https://mistral.ai/)",
      "icon": "mistral.jpg",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "mistral",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|mistral"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "md:The model to generate the completion. \n\n ## Pricing and rate limits\n\n**Pay-as-you-go**\n\nThe prices listed below are exclusive of VAT.\n\n### Chat Completions API\n\n| Model           | Endpoint                | Input (USD)         | Output (USD)         |\n|-----------------|-------------------------|---------------------|----------------------|\n| Mistral 7B      | open-mistral-7b         | 0.25$ / 1M tokens   | 0.25$ / 1M tokens    |\n| Mixtral 8x7B    | open-mixtral-8x7b       | 0.7$ / 1M tokens    | 0.7$ / 1M tokens     |\n| Mistral Small   | mistral-small-latest     | 2$ / 1M tokens     | 6$ / 1M tokens       |\n| Mistral Medium  | mistral-medium-latest    | 2.7$ / 1M tokens   | 8.1$ / 1M tokens     |\n| Mistral Large   | mistral-large-latest     | 8$ / 1M tokens     | 24$ / 1M tokens      |\n",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "mistral-small-latest",
          "dataProxy": "llm|mistral_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_ollama",
      "title": "Ollama",
      "description": "Chat with Ollama, learn more : [ollama.ai](https://ollama.ai/)",
      "icon": "ollama.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "ollama",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|ollama"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama2:latest",
          "dataProxy": "llm|ollama_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_cohere",
      "title": "Cohere AI",
      "description": "Chat using [Cohere](cohere.com), get [API KEY](https://dashboard.cohere.com/)",
      "icon": "cohere.jpg",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "cohere",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|cohere"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "model",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "command-r",
          "dataProxy": "llm|cohere_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_perplexity",
      "title": "Perplexity",
      "description": "Chat with Perplexity, learn more: [perplexity.ai](https://www.perplexity.ai/settings/api)",
      "icon": "perplexity.png",
      "mode": "no-view",
      "commandType": "provider",
      "targetCommand": "llm|chat_open_ai",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "perplexity",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|perplexity"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "sonar",
          "dataProxy": "llm|perplexity_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        },
        {
          "name": "frequencyPenalty",
          "description": "frequencyPenalty",
          "type": "textfield",
          "required": false,
          "title": "frequencyPenalty",
          "default": "1",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "siliconflow",
      "title": "SiliconFlow",
      "description": "Chat with SiliconFlow , learn more : [SiliconFlow developers](https://siliconflow.cn/)",
      "icon": "siliconflow.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "siliconflow",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|siliconflow"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "Qwen/Qwen2-7B-Instruct",
          "dataProxy": "llm|silliconflow_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_lm_studio",
      "title": "LM Studio",
      "description": "Chat with LM Studio, learn more : [lmstudio.ai](https://lmstudio.ai/)",
      "icon": "lm_studio.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "lm_studio",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|lm_studio"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "title": "Model Name",
          "dataProxy": "llm|lmstudio_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_openrouter",
      "title": "OpenRouter",
      "description": "Chat with OpenRouter, learn more: [openrouter.ai](https://openrouter.ai/)",
      "icon": "openrouter.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "openrouter",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|openrouter"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "anthropic/claude-3.5-sonnet",
          "dataProxy": "llm|openrouter_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_fireworks",
      "title": "Fireworks",
      "description": "Chat with Fireworks, learn more : [fireworks.ai](https://fireworks.ai), get all models from [fireworks models](https://fireworks.ai/models/fireworks/mixtral-8x7b-instruct)",
      "icon": "fireworks.jpg",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "fireworks",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|fireworks"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "accounts/fireworks/models/deepseek-v3-0324",
          "dataProxy": "llm|fireworks_models",
          "dataCustomTitle": "Add Custom Model"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_cloudflare",
      "title": "Cloudflare Workers AI",
      "description": "Chat with Cloudflare Workers AI, learn more : [workers-ai](https://developers.cloudflare.com/workers-ai/)",
      "icon": "cloudflare.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "cloudflare",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|cloudflare"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "@cf/meta/llama-2-7b-chat-int8",
          "dataProxy": "llm|cloudflare_models"
        }
      ]
    },
    {
      "name": "chat_together_ai",
      "title": "Together AI",
      "description": "Chat with together.ai, learn more : [together.ai](https://docs.together.ai/docs/quickstart), get chat models from [together models](https://docs.together.ai/docs/inference-models)",
      "icon": "together.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "together_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|together_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "meta-llama/Llama-3.2-3B-Instruct-Turbo",
          "dataProxy": "llm|together_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_poe",
      "title": "Poe",
      "description": "Chat with Poe, learn more : [poe.com](https://poe.com)",
      "icon": "poe.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "poe",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|poe"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "claude-3.5-sonnet",
          "dataProxy": "llm|poe_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_moonshot_ai",
      "title": "MoonShot AI",
      "description": "Chat with moonshot ai, learn more : [moonshot.cn](https://www.moonshot.cn/)",
      "icon": "moonshot.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "moonshot_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|moonshot_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "kimi-k2-0711-preview",
          "dataProxy": "llm|moonshot_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_yii",
      "title": "Yi",
      "description": "Chat with Yi , learn more : [Yi developers](https://platform.lingyiwanwu.com/)",
      "icon": "yi.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "yii",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|yii"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "yi-34b-chat-0205",
          "data": [
            {
              "title": "yi-large",
              "value": "yi-large",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-medium",
              "value": "yi-medium",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-vision",
              "value": "yi-vision",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-large-turbo",
              "value": "yi-large-turbo",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-34b-chat-0205",
              "value": "yi-34b-chat-0205",
              "context": 4000,
              "inputPrice": 0.0005,
              "outputPrice": 0.0015,
              "maxTokens": 3000
            },
            {
              "title": "yi-34b-chat-200k",
              "value": "yi-34b-chat-200k",
              "context": 200000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "maxTokens": 4000
            },
            {
              "title": "yi-vl-plus",
              "value": "yi-vl-plus",
              "context": 4000,
              "inputPrice": 0.01,
              "outputPrice": 0.03,
              "visionEnable": true,
              "maxTokens": 2000
            }
          ]
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_dashscope",
      "title": "DashScope",
      "description": "Chat with é˜¿é‡Œäº‘ç™¾ç‚¼ , learn more : [é˜¿é‡Œäº‘ç™¾ç‚¼](https://bailian.console.aliyun.com/#/home)",
      "icon": "dashscope.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "dashscope",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|dashscope"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "qwen-max-latest",
          "dataProxy": "llm|dashscope_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_sambanova",
      "title": "Sambanova AI",
      "description": "Chat with sambanova.ai, learn more : [sambanova.ai](https://cloud.sambanova.ai/pricing)",
      "icon": "sambanova.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "sambanova",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|sambanova"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "Meta-Llama-3.2-3B-Instruct",
          "dataProxy": "llm|sambanova_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_cerebras",
      "title": "Cerebras AI",
      "description": "Chat with cerebras.ai, learn more : [cerebras.ai](https://cerebras.ai/)",
      "icon": "cerebras.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "cerebras",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|cerebras"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "llama3.1-8b",
          "dataProxy": "llm|cerebras_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_arli",
      "title": "Arli AI",
      "description": "Chat with arli.ai, learn more : [arliai.com](https://arliai.com/)",
      "icon": "arli.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "arli",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|arli"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "(TRIAL) Llama-3.1-70B-ArliAI-RPMax-v1.1",
          "dataProxy": "llm|arli_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "chat_straico",
      "title": "Straico AI",
      "description": "Chat with straico.com, learn more : [straico.com](https://straico.com/)",
      "icon": "straico.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "straico",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|straico"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "openai/gpt-4o-mini",
          "dataProxy": "llm|straico_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "302_ai",
      "title": "302.AI",
      "description": "Chat with 302.ai, learn more : [302.ai](https://share.302.ai/jMFReu)",
      "icon": "302.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "302_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|302_ai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "qwen2.5-3b-instruct",
          "dataProxy": "llm|302_ai_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 1,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 1
            },
            {
              "title": "high",
              "value": 1.5
            },
            {
              "title": "maximum",
              "value": 2
            }
          ]
        }
      ]
    },
    {
      "name": "ollama_models",
      "title": "Ollama Models",
      "commandType": "function_command",
      "description": "get ollama model list",
      "icon": "ollama.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "ollama",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|ollama"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "openrouter_models",
      "commandType": "function_command",
      "title": "openrouter Models",
      "description": "get openrouter model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "perplexity_models",
      "targetCommand": "llm|fetch_models",
      "commandType": "function_command",
      "title": "perplexity Models",
      "description": "get perplexity model list",
      "icon": "perplexity.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "title": "Url",
          "description": "perplexity api url",
          "type": "textfield",
          "required": false,
          "default": "https://file.enconvo.com/modles/perplexity.json"
        }
      ]
    },
    {
      "name": "groq_models",
      "title": "groq Models",
      "commandType": "function_command",
      "description": "get groq model list",
      "icon": "groq.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "groq",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|groq"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "x_ai_models",
      "title": "X.AI Models",
      "description": "get X.AI model list",
      "commandType": "function_command",
      "icon": "x_ai.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "x_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|x_ai"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "moonshot_models",
      "title": "Moonshot Models",
      "description": "get Moonshot model list",
      "commandType": "function_command",
      "icon": "moonshot.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "moonshot_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|moonshot_ai"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "straico_models",
      "title": "Straico Models",
      "description": "get Straico model list",
      "commandType": "function_command",
      "icon": "straico.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "straico",
          "defaultProxy": "llm|chat_straico",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|straico"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "type",
          "description": "How to get api key? [ðŸ”‘here](https://straico.com/)",
          "type": "textfield",
          "required": false,
          "title": "Type",
          "default": "chat"
        }
      ]
    },
    {
      "name": "gemini_models",
      "title": "Gemini Models",
      "description": "get Gemini model list",
      "commandType": "function_command",
      "icon": "gemini.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "gemini",
          "extensionType": "credentials-provider",
          "defaultProxy": "llm|chat_google",
          "extensionFilter": {
            "targetCommands": [
              "credentials|gemini"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "deepseek_models",
      "title": "DeepSeek Models",
      "description": "get DeepSeek model list",
      "commandType": "function_command",
      "icon": "deepseek.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": []
    },
    {
      "name": "dashscope_models",
      "title": "DashScope Models",
      "description": "get DashScope model list",
      "commandType": "function_command",
      "icon": "dashscope.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "apiKey",
          "description": "How to get api key? [ðŸ”‘here](https://bailian.console.aliyun.com/#/home)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "credentials|dashscope",
          "placeholder": "sk-********"
        },
        {
          "name": "baseUrl",
          "description": "DashScope api base url",
          "type": "textfield",
          "required": false,
          "title": "OpenAI Api Base Url",
          "default": "https://dashscope.aliyuncs.com/compatible-mode/v1",
          "placeholder": "OpenAI Api Base Url",
          "visibility": "hidden"
        }
      ]
    },
    {
      "name": "sambanova_models",
      "title": "Sambanova Models",
      "description": "get Sambanova model list",
      "commandType": "function_command",
      "icon": "sambanova.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "sambanova",
          "defaultProxy": "llm|chat_sambanova",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|sambanova"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "cerebras_models",
      "title": "Cerebras Models",
      "description": "get Cerebras model list",
      "commandType": "function_command",
      "icon": "cerebras.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "cerebras",
          "defaultProxy": "llm|chat_cerebras",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|cerebras"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "mistral_models",
      "title": "Mistral Models",
      "description": "get Mistral model list",
      "commandType": "function_command",
      "icon": "mistral.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "apiKey",
          "description": "How to get api key? [ðŸ”‘here](https://console.mistral.ai/api-keys/)",
          "type": "password",
          "required": false,
          "title": "API Key",
          "default": "",
          "defaultProxy": "credentials|mistral",
          "placeholder": "Mistral AI Api Key"
        },
        {
          "name": "baseUrl",
          "description": "Mixtral api base url",
          "type": "textfield",
          "required": false,
          "title": "BaseUrl",
          "default": "https://api.mistral.ai/v1",
          "defaultProxy": "credentials|mistral",
          "placeholder": "Mixtral Api Base Url"
        }
      ]
    },
    {
      "name": "poe_models",
      "title": "Poe Models",
      "description": "get Poe model list",
      "commandType": "function_command",
      "icon": "poe.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "poe",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|poe"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "together_models",
      "targetCommand": "llm|fetch_models",
      "title": "together Models",
      "commandType": "function_command",
      "description": "get together model list",
      "icon": "together.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "title": "Url",
          "name": "url",
          "description": "together api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/together/v1/models"
        }
      ]
    },
    {
      "name": "fireworks_models",
      "title": "fireworks Models",
      "commandType": "function_command",
      "description": "get fireworks model list",
      "icon": "fireworks.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "fireworks",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|fireworks"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "302_ai_models",
      "title": "302 AI Models",
      "commandType": "function_command",
      "description": "get 302 AI model list",
      "icon": "302.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "302_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|302_ai"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "fetch_models",
      "title": "fetch Models",
      "description": "get models list",
      "icon": "together.png",
      "commandType": "function_command",
      "showInCommandList": false,
      "mode": "no-view"
    },
    {
      "name": "enconvo_models",
      "targetCommand": "llm|fetch_models",
      "title": "enconvo Models",
      "commandType": "function_command",
      "description": "get enconvo model list",
      "icon": "enconvo.png",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "enconvo api url",
          "type": "textfield",
          "required": false,
          "default": "https://file.enconvo.com/modles/enconvo.json"
        }
      ]
    },
    {
      "name": "openai_models",
      "title": "openai Models",
      "description": "get openai model list",
      "icon": "openai.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "open_ai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|open_ai"
            ]
          },
          "title": "Credential Provider",
          "defaultProxy": "llm|chat_open_ai"
        }
      ]
    },
    {
      "name": "lmstudio_models",
      "title": "lmstudio Models",
      "description": "get lmstudio model list",
      "icon": "lmstudio.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "targetCommand": "llm|openai_models",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "lm_studio",
          "defaultProxy": "llm|chat_lm_studio",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|lm_studio"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "azureai_models",
      "title": "azureai Models",
      "targetCommand": "llm|fetch_models",
      "description": "get azureai model list",
      "icon": "azureai.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "openai api url",
          "type": "textfield",
          "required": false,
          "default": "https://file.enconvo.com/modles/openai.json"
        }
      ]
    },
    {
      "name": "cloudflare_models",
      "title": "cloudflare Models",
      "targetCommand": "llm|fetch_models",
      "description": "get cloudflare model list",
      "icon": "cloudflare.png",
      "commandType": "function_command",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "cloudflare api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/cloudflare/v1/models"
        }
      ]
    },
    {
      "name": "cohere_models",
      "title": "cohere Models",
      "targetCommand": "llm|fetch_models",
      "description": "get cohere model list",
      "icon": "cohere.jpg",
      "commandType": "function_command",
      "showInCommandList": false,
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "title": "Url",
          "description": "cohere api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/cohere/v1/models"
        }
      ]
    },
    {
      "name": "silliconflow_models",
      "title": "silliconflow_models",
      "targetCommand": "llm|fetch_models",
      "description": "get silliconflow model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "url",
          "description": "silliconflow api url",
          "type": "textfield",
          "required": false,
          "default": "https://api-v.enconvo.com/siliconflow/v1/models"
        }
      ]
    },
    {
      "name": "chat_aimagicx",
      "title": "AIMagicX",
      "description": "Chat with AIMagicX, learn more : [aimagicx.com](https://aimagicx.com/)",
      "icon": "aimagicx.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "aimagicx",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|aimagicx"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "4o-mini",
          "dataProxy": "llm|aimagicx_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 0.7,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 0.7
            },
            {
              "title": "high",
              "value": 1
            }
          ]
        },
        {
          "name": "maxTokens",
          "description": "The maximum number of tokens to generate in the completion.",
          "type": "textfield",
          "required": false,
          "title": "Max Tokens",
          "default": "4096"
        }
      ]
    },
    {
      "name": "aimagicx_models",
      "title": "aimagicx_models",
      "targetCommand": "llm|aimagicx_models",
      "description": "get AIMagicX model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "aimagicx",
          "extensionType": "credentials-provider",
          "defaultProxy": "llm|chat_aimagicx",
          "extensionFilter": {
            "targetCommands": [
              "credentials|aimagicx"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "chat_1minai",
      "title": "1min AI",
      "description": "Chat with 1min AI, learn more : [1min.ai](https://1min.ai/)",
      "icon": "1minai.png",
      "mode": "no-view",
      "commandType": "provider",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "1minai",
          "extensionType": "credentials-provider",
          "extensionFilter": {
            "targetCommands": [
              "credentials|1minai"
            ]
          },
          "title": "Credential Provider"
        },
        {
          "name": "modelName",
          "description": "The model to generate the completion.",
          "type": "dropdown",
          "required": false,
          "title": "Model Name",
          "default": "gpt-4o-mini",
          "dataProxy": "llm|1minai_models"
        },
        {
          "name": "temperature",
          "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "type": "dropdown",
          "required": false,
          "title": "Temperature",
          "default": 0.7,
          "data": [
            {
              "title": "none",
              "value": 0
            },
            {
              "title": "low",
              "value": 0.5
            },
            {
              "title": "medium",
              "value": 0.7
            },
            {
              "title": "high",
              "value": 1
            }
          ]
        },
        {
          "name": "maxTokens",
          "description": "The maximum number of tokens to generate in the completion.",
          "type": "textfield",
          "required": false,
          "title": "Max Tokens",
          "default": "4096"
        }
      ]
    },
    {
      "name": "1minai_models",
      "title": "1minai_models",
      "targetCommand": "llm|1minai_models",
      "description": "get 1min AI model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "1minai",
          "extensionType": "credentials-provider",
          "defaultProxy": "llm|chat_1minai",
          "extensionFilter": {
            "targetCommands": [
              "credentials|1minai"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    },
    {
      "name": "arli_models",
      "title": "arli_models",
      "targetCommand": "llm|fetch_models",
      "description": "get arli model list",
      "icon": "openrouter.png",
      "showInCommandList": false,
      "commandType": "function_command",
      "mode": "no-view",
      "preferences": [
        {
          "name": "credentials",
          "description": "The key management provider to use",
          "type": "extension",
          "required": false,
          "default": "arli",
          "extensionType": "credentials-provider",
          "defaultProxy": "llm|chat_arli",
          "extensionFilter": {
            "targetCommands": [
              "credentials|arli"
            ]
          },
          "title": "Credential Provider"
        }
      ]
    }
  ],
  "dependencies": {
    "@ai-sdk/openai": "^2.0.15",
    "@anthropic-ai/sdk": "^0.60.0",
    "@enconvo/api": "link:/Users/ysnows/Documents/Project/enconvo.nodejs/enconvo_api",
    "@google/genai": "^1.0.0",
    "@langchain/cloudflare": "^0.1.0",
    "@langchain/cohere": "^0.3.1",
    "@langchain/community": "^0.3.19",
    "@langchain/core": "^0.3.23",
    "@lmnr-ai/lmnr": "^0.4.42",
    "@types/wav": "^1.0.4",
    "ai": "^5.0.15",
    "axios": "^1.7.9",
    "form-data": "^4.0.1",
    "langchain": "^0.3.7",
    "langsmith": "^0.3.28",
    "mime": "^4.0.6",
    "ollama": "^0.5.17",
    "open": "^10.2.0",
    "openai": "^5.12.2",
    "pkce": "link:@openauthjs/openauth/pkce",
    "wav": "^1.0.2",
    "zod": "^4.0.17"
  },
  "devDependencies": {
    "@types/node": "^22.10.2",
    "eslint": "^9.17.0",
    "prettier": "^3.4.2",
    "tsup": "^8.3.5",
    "typescript": "^5.7.2"
  },
  "scripts": {
    "lint": "eslint src",
    "lint:fix": "npm run lint --fix",
    "format": "prettier --write \"**/*.ts\"",
    "format:check": "prettier --list-different \"**/*.ts\"",
    "build": "enconvo",
    "dev": "enconvo --dev"
  },
  "minAppVersion": "1.8.8"
}
